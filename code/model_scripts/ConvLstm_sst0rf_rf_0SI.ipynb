{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19c9307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (2311, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (2311, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (2311, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (2311, 1, 25, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 11:06:09.316403: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 7, 25, 39, 4)      880       \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 7, 25, 39, 8)      3488      \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 7, 25, 39, 8)      4640      \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 7, 25, 39, 16)     13888     \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 25, 39, 16)        18496     \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 25, 39, 15)        2175      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 39, 1)         136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,703\n",
      "Trainable params: 43,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/300\n",
      "42/42 [==============================] - 54s 966ms/step - loss: 1215.8787 - mae: 32.1800 - val_loss: 1102.2413 - val_mae: 31.6896 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "42/42 [==============================] - 40s 947ms/step - loss: 1184.3640 - mae: 31.6811 - val_loss: 981.7788 - val_mae: 29.7208 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "42/42 [==============================] - 39s 924ms/step - loss: 850.2504 - mae: 25.7115 - val_loss: 547.3157 - val_mae: 21.1317 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 548.9214 - mae: 19.1610 - val_loss: 361.8747 - val_mae: 16.1996 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "42/42 [==============================] - 39s 924ms/step - loss: 390.4131 - mae: 14.6103 - val_loss: 235.6978 - val_mae: 11.8179 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "42/42 [==============================] - 42s 993ms/step - loss: 285.6018 - mae: 10.8442 - val_loss: 161.9232 - val_mae: 8.3899 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "42/42 [==============================] - 37s 871ms/step - loss: 231.2588 - mae: 8.6984 - val_loss: 132.2870 - val_mae: 6.9973 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "42/42 [==============================] - 41s 980ms/step - loss: 211.4438 - mae: 8.1888 - val_loss: 124.4465 - val_mae: 7.0000 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "42/42 [==============================] - 41s 961ms/step - loss: 206.3848 - mae: 8.3066 - val_loss: 123.6036 - val_mae: 7.2163 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "42/42 [==============================] - 41s 976ms/step - loss: 205.5273 - mae: 8.3985 - val_loss: 123.5357 - val_mae: 7.2820 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "42/42 [==============================] - 36s 845ms/step - loss: 205.3443 - mae: 8.4398 - val_loss: 123.5482 - val_mae: 7.3289 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "42/42 [==============================] - 35s 828ms/step - loss: 205.1898 - mae: 8.4318 - val_loss: 123.3967 - val_mae: 7.3102 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "42/42 [==============================] - 37s 888ms/step - loss: 205.0653 - mae: 8.4222 - val_loss: 123.2631 - val_mae: 7.2996 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "42/42 [==============================] - 37s 870ms/step - loss: 204.9298 - mae: 8.4252 - val_loss: 123.1900 - val_mae: 7.3169 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "42/42 [==============================] - 37s 882ms/step - loss: 204.7873 - mae: 8.4316 - val_loss: 123.0596 - val_mae: 7.3129 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "42/42 [==============================] - 38s 910ms/step - loss: 204.6503 - mae: 8.4418 - val_loss: 122.9446 - val_mae: 7.3155 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "42/42 [==============================] - 37s 892ms/step - loss: 204.4937 - mae: 8.4119 - val_loss: 122.8074 - val_mae: 7.3117 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "42/42 [==============================] - 39s 938ms/step - loss: 204.3328 - mae: 8.4168 - val_loss: 122.5991 - val_mae: 7.2861 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "42/42 [==============================] - 39s 923ms/step - loss: 204.1588 - mae: 8.4019 - val_loss: 122.4326 - val_mae: 7.2751 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "42/42 [==============================] - 35s 837ms/step - loss: 204.0112 - mae: 8.4030 - val_loss: 122.2823 - val_mae: 7.2727 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "42/42 [==============================] - 37s 882ms/step - loss: 203.8288 - mae: 8.3881 - val_loss: 122.1021 - val_mae: 7.2605 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "42/42 [==============================] - 38s 917ms/step - loss: 203.6792 - mae: 8.3771 - val_loss: 121.9414 - val_mae: 7.2563 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "42/42 [==============================] - 38s 907ms/step - loss: 203.4730 - mae: 8.3657 - val_loss: 121.7742 - val_mae: 7.2491 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "42/42 [==============================] - 38s 914ms/step - loss: 203.2760 - mae: 8.3671 - val_loss: 121.5824 - val_mae: 7.2433 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "42/42 [==============================] - 40s 959ms/step - loss: 203.1186 - mae: 8.3842 - val_loss: 121.3718 - val_mae: 7.2270 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "42/42 [==============================] - 38s 914ms/step - loss: 202.8896 - mae: 8.3247 - val_loss: 121.1817 - val_mae: 7.2221 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "42/42 [==============================] - 38s 914ms/step - loss: 202.6839 - mae: 8.3499 - val_loss: 120.9780 - val_mae: 7.2130 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "42/42 [==============================] - 40s 955ms/step - loss: 202.4597 - mae: 8.3361 - val_loss: 120.7591 - val_mae: 7.2008 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "42/42 [==============================] - 35s 839ms/step - loss: 202.2492 - mae: 8.3120 - val_loss: 120.6440 - val_mae: 7.2249 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "42/42 [==============================] - 37s 876ms/step - loss: 201.9973 - mae: 8.3250 - val_loss: 120.2363 - val_mae: 7.1473 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "42/42 [==============================] - 41s 966ms/step - loss: 201.6377 - mae: 8.2841 - val_loss: 121.7991 - val_mae: 7.2284 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "42/42 [==============================] - 38s 910ms/step - loss: 200.5726 - mae: 8.2500 - val_loss: 120.2897 - val_mae: 7.2202 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "42/42 [==============================] - 38s 896ms/step - loss: 199.5596 - mae: 8.2479 - val_loss: 119.7436 - val_mae: 7.2332 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "42/42 [==============================] - 39s 922ms/step - loss: 198.7847 - mae: 8.2167 - val_loss: 119.8491 - val_mae: 7.2318 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "42/42 [==============================] - 39s 923ms/step - loss: 198.0626 - mae: 8.1909 - val_loss: 118.6126 - val_mae: 7.1862 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "42/42 [==============================] - 39s 922ms/step - loss: 197.2560 - mae: 8.1642 - val_loss: 118.3090 - val_mae: 7.2290 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "42/42 [==============================] - 42s 992ms/step - loss: 196.2182 - mae: 8.1588 - val_loss: 117.6397 - val_mae: 7.1484 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "42/42 [==============================] - 41s 972ms/step - loss: 193.2338 - mae: 7.9376 - val_loss: 113.8648 - val_mae: 6.7860 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "42/42 [==============================] - 39s 941ms/step - loss: 188.6939 - mae: 7.8558 - val_loss: 111.2523 - val_mae: 6.8674 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "42/42 [==============================] - 41s 985ms/step - loss: 184.2071 - mae: 7.7504 - val_loss: 107.4391 - val_mae: 6.7947 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "42/42 [==============================] - 47s 1s/step - loss: 176.1429 - mae: 7.4712 - val_loss: 106.8118 - val_mae: 6.9281 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "42/42 [==============================] - 37s 878ms/step - loss: 171.7367 - mae: 7.4101 - val_loss: 101.7404 - val_mae: 6.5518 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "42/42 [==============================] - 38s 906ms/step - loss: 168.2482 - mae: 7.3122 - val_loss: 100.1919 - val_mae: 6.6733 - lr: 1.0000e-04\n",
      "Epoch 44/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 38s 911ms/step - loss: 165.4832 - mae: 7.2846 - val_loss: 96.6151 - val_mae: 6.3706 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "42/42 [==============================] - 36s 867ms/step - loss: 163.2008 - mae: 7.2302 - val_loss: 96.9712 - val_mae: 6.6042 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 159.5351 - mae: 7.1276 - val_loss: 92.0772 - val_mae: 6.2450 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "42/42 [==============================] - 40s 962ms/step - loss: 157.1793 - mae: 7.0675 - val_loss: 89.9218 - val_mae: 6.2622 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "42/42 [==============================] - 37s 891ms/step - loss: 154.0591 - mae: 6.9760 - val_loss: 87.9003 - val_mae: 6.1416 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "42/42 [==============================] - 39s 933ms/step - loss: 151.4593 - mae: 6.9101 - val_loss: 85.0844 - val_mae: 5.9887 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "42/42 [==============================] - 47s 1s/step - loss: 148.9910 - mae: 6.8496 - val_loss: 83.0736 - val_mae: 6.0077 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "42/42 [==============================] - 39s 943ms/step - loss: 146.5652 - mae: 6.7727 - val_loss: 82.5220 - val_mae: 6.1043 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "42/42 [==============================] - 42s 998ms/step - loss: 143.9844 - mae: 6.7168 - val_loss: 79.0721 - val_mae: 5.8283 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "42/42 [==============================] - 39s 930ms/step - loss: 140.7760 - mae: 6.6046 - val_loss: 75.7864 - val_mae: 5.6946 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "42/42 [==============================] - 40s 946ms/step - loss: 137.7919 - mae: 6.5105 - val_loss: 73.9502 - val_mae: 5.6711 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 133.6152 - mae: 6.3916 - val_loss: 70.4257 - val_mae: 5.5152 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "42/42 [==============================] - 38s 892ms/step - loss: 130.3049 - mae: 6.3198 - val_loss: 70.5558 - val_mae: 5.5213 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "42/42 [==============================] - 38s 898ms/step - loss: 126.6585 - mae: 6.1953 - val_loss: 65.2088 - val_mae: 5.4512 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "42/42 [==============================] - 39s 925ms/step - loss: 121.5497 - mae: 6.0642 - val_loss: 62.1350 - val_mae: 5.2543 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "42/42 [==============================] - 40s 960ms/step - loss: 116.6660 - mae: 5.9094 - val_loss: 60.2293 - val_mae: 5.2467 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "42/42 [==============================] - 36s 850ms/step - loss: 112.1487 - mae: 5.7821 - val_loss: 56.1990 - val_mae: 5.1142 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "42/42 [==============================] - 35s 835ms/step - loss: 108.5009 - mae: 5.6907 - val_loss: 53.9826 - val_mae: 4.9914 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "42/42 [==============================] - 36s 860ms/step - loss: 104.5894 - mae: 5.5549 - val_loss: 51.9096 - val_mae: 4.9381 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "42/42 [==============================] - 40s 948ms/step - loss: 101.6959 - mae: 5.4698 - val_loss: 50.2831 - val_mae: 4.8233 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "42/42 [==============================] - 42s 997ms/step - loss: 98.8659 - mae: 5.3864 - val_loss: 48.8739 - val_mae: 4.8170 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "42/42 [==============================] - 48s 1s/step - loss: 96.4859 - mae: 5.3147 - val_loss: 48.0804 - val_mae: 4.7922 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "42/42 [==============================] - 39s 930ms/step - loss: 94.8125 - mae: 5.2892 - val_loss: 46.0998 - val_mae: 4.6494 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 92.3421 - mae: 5.1946 - val_loss: 44.8757 - val_mae: 4.6057 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 90.9975 - mae: 5.1551 - val_loss: 44.0597 - val_mae: 4.5640 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "42/42 [==============================] - 35s 840ms/step - loss: 88.9186 - mae: 5.0899 - val_loss: 42.7703 - val_mae: 4.4873 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "42/42 [==============================] - 35s 830ms/step - loss: 87.3523 - mae: 5.0434 - val_loss: 42.0339 - val_mae: 4.4026 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "42/42 [==============================] - 40s 957ms/step - loss: 86.0261 - mae: 4.9946 - val_loss: 41.1226 - val_mae: 4.3542 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "42/42 [==============================] - 37s 890ms/step - loss: 84.2673 - mae: 4.9255 - val_loss: 39.9746 - val_mae: 4.3049 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 82.8113 - mae: 4.8786 - val_loss: 39.2526 - val_mae: 4.2651 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "42/42 [==============================] - 39s 924ms/step - loss: 81.2783 - mae: 4.8346 - val_loss: 38.5103 - val_mae: 4.1948 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "42/42 [==============================] - 53s 1s/step - loss: 79.7760 - mae: 4.7763 - val_loss: 37.8702 - val_mae: 4.1582 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 78.4024 - mae: 4.7178 - val_loss: 36.9510 - val_mae: 4.1474 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 76.6575 - mae: 4.6552 - val_loss: 35.8907 - val_mae: 4.0819 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "42/42 [==============================] - 38s 906ms/step - loss: 75.0628 - mae: 4.5860 - val_loss: 35.4912 - val_mae: 4.0645 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "42/42 [==============================] - 39s 920ms/step - loss: 73.3225 - mae: 4.5173 - val_loss: 34.4315 - val_mae: 3.9851 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "42/42 [==============================] - 40s 951ms/step - loss: 71.7831 - mae: 4.4555 - val_loss: 34.5101 - val_mae: 4.0132 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 69.9306 - mae: 4.3921 - val_loss: 32.6463 - val_mae: 3.8516 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "42/42 [==============================] - 42s 998ms/step - loss: 68.3497 - mae: 4.3398 - val_loss: 31.4266 - val_mae: 3.7720 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "42/42 [==============================] - 39s 920ms/step - loss: 66.5701 - mae: 4.2360 - val_loss: 30.8614 - val_mae: 3.7613 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "42/42 [==============================] - 39s 930ms/step - loss: 64.7581 - mae: 4.1485 - val_loss: 29.3000 - val_mae: 3.6391 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "42/42 [==============================] - 48s 1s/step - loss: 63.2908 - mae: 4.0579 - val_loss: 28.3577 - val_mae: 3.5738 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 61.7265 - mae: 3.9761 - val_loss: 27.3820 - val_mae: 3.4840 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "42/42 [==============================] - 39s 932ms/step - loss: 61.0117 - mae: 3.9576 - val_loss: 27.9478 - val_mae: 3.5810 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "42/42 [==============================] - 38s 903ms/step - loss: 59.0075 - mae: 3.8386 - val_loss: 25.3038 - val_mae: 3.3329 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "42/42 [==============================] - 38s 896ms/step - loss: 57.5953 - mae: 3.7549 - val_loss: 24.5187 - val_mae: 3.2469 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 56.8772 - mae: 3.7070 - val_loss: 24.5315 - val_mae: 3.2247 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "42/42 [==============================] - 41s 975ms/step - loss: 55.6477 - mae: 3.6451 - val_loss: 23.6226 - val_mae: 3.1794 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "42/42 [==============================] - 38s 914ms/step - loss: 54.4495 - mae: 3.5853 - val_loss: 22.9135 - val_mae: 3.1047 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "42/42 [==============================] - 39s 938ms/step - loss: 54.0022 - mae: 3.5754 - val_loss: 22.6868 - val_mae: 3.1213 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 53.1219 - mae: 3.5229 - val_loss: 22.1366 - val_mae: 3.0382 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "42/42 [==============================] - 39s 938ms/step - loss: 51.8994 - mae: 3.4432 - val_loss: 22.3822 - val_mae: 3.0880 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "42/42 [==============================] - 36s 866ms/step - loss: 51.2571 - mae: 3.4224 - val_loss: 21.4248 - val_mae: 2.9916 - lr: 1.0000e-04\n",
      "Epoch 97/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 42s 991ms/step - loss: 50.7554 - mae: 3.4023 - val_loss: 21.6889 - val_mae: 2.9737 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 49.9688 - mae: 3.3701 - val_loss: 20.6439 - val_mae: 2.9184 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 49.0570 - mae: 3.3230 - val_loss: 20.1587 - val_mae: 2.8797 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "42/42 [==============================] - 41s 977ms/step - loss: 48.5286 - mae: 3.2998 - val_loss: 20.0854 - val_mae: 2.8709 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "42/42 [==============================] - 41s 985ms/step - loss: 47.8884 - mae: 3.2781 - val_loss: 20.0222 - val_mae: 2.8636 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "42/42 [==============================] - 39s 920ms/step - loss: 47.2678 - mae: 3.2399 - val_loss: 19.4305 - val_mae: 2.8115 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "42/42 [==============================] - 42s 997ms/step - loss: 46.6457 - mae: 3.2064 - val_loss: 19.2678 - val_mae: 2.8046 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "42/42 [==============================] - 38s 911ms/step - loss: 46.0907 - mae: 3.1883 - val_loss: 19.3591 - val_mae: 2.8000 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "42/42 [==============================] - 40s 960ms/step - loss: 45.6915 - mae: 3.1712 - val_loss: 19.7840 - val_mae: 2.8009 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "42/42 [==============================] - 42s 995ms/step - loss: 45.3638 - mae: 3.1756 - val_loss: 18.3450 - val_mae: 2.7352 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "42/42 [==============================] - 40s 948ms/step - loss: 44.7595 - mae: 3.1481 - val_loss: 20.2023 - val_mae: 2.8949 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "42/42 [==============================] - 53s 1s/step - loss: 44.2121 - mae: 3.1158 - val_loss: 18.6709 - val_mae: 2.7623 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "42/42 [==============================] - 41s 971ms/step - loss: 43.6199 - mae: 3.0781 - val_loss: 17.4505 - val_mae: 2.6587 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "42/42 [==============================] - 38s 910ms/step - loss: 43.2673 - mae: 3.0651 - val_loss: 17.4288 - val_mae: 2.6460 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 42.7459 - mae: 3.0385 - val_loss: 17.7558 - val_mae: 2.7033 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "42/42 [==============================] - 41s 967ms/step - loss: 42.3660 - mae: 3.0183 - val_loss: 17.1225 - val_mae: 2.6300 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "42/42 [==============================] - 39s 928ms/step - loss: 41.9760 - mae: 3.0088 - val_loss: 16.7902 - val_mae: 2.5892 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "42/42 [==============================] - 41s 967ms/step - loss: 41.6198 - mae: 2.9990 - val_loss: 16.6856 - val_mae: 2.5697 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "42/42 [==============================] - 40s 958ms/step - loss: 41.3602 - mae: 2.9958 - val_loss: 16.3945 - val_mae: 2.5420 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 40.7262 - mae: 2.9493 - val_loss: 16.2005 - val_mae: 2.5488 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 40.4363 - mae: 2.9381 - val_loss: 16.1671 - val_mae: 2.5455 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "42/42 [==============================] - 37s 875ms/step - loss: 40.1385 - mae: 2.9234 - val_loss: 15.7896 - val_mae: 2.5024 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "42/42 [==============================] - 39s 926ms/step - loss: 39.8272 - mae: 2.9216 - val_loss: 15.7009 - val_mae: 2.4941 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 39.1975 - mae: 2.8782 - val_loss: 15.4048 - val_mae: 2.4630 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "42/42 [==============================] - 38s 909ms/step - loss: 38.8827 - mae: 2.8640 - val_loss: 15.5674 - val_mae: 2.4856 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "42/42 [==============================] - 37s 895ms/step - loss: 38.6611 - mae: 2.8604 - val_loss: 15.2048 - val_mae: 2.4405 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "42/42 [==============================] - 48s 1s/step - loss: 38.1908 - mae: 2.8270 - val_loss: 14.8532 - val_mae: 2.4156 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "42/42 [==============================] - 41s 968ms/step - loss: 37.8808 - mae: 2.8224 - val_loss: 14.7400 - val_mae: 2.3963 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "42/42 [==============================] - 46s 1s/step - loss: 37.5153 - mae: 2.7998 - val_loss: 14.6223 - val_mae: 2.4039 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 37.3330 - mae: 2.8020 - val_loss: 14.8367 - val_mae: 2.4508 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "42/42 [==============================] - 48s 1s/step - loss: 37.0846 - mae: 2.7949 - val_loss: 14.2930 - val_mae: 2.3588 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "42/42 [==============================] - 40s 949ms/step - loss: 36.7063 - mae: 2.7709 - val_loss: 14.1115 - val_mae: 2.3408 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "42/42 [==============================] - 39s 922ms/step - loss: 36.2818 - mae: 2.7393 - val_loss: 14.0664 - val_mae: 2.3450 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "42/42 [==============================] - 38s 917ms/step - loss: 36.3319 - mae: 2.7615 - val_loss: 13.9224 - val_mae: 2.3281 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "42/42 [==============================] - 41s 981ms/step - loss: 35.7965 - mae: 2.7197 - val_loss: 13.7053 - val_mae: 2.3063 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 35.5880 - mae: 2.7187 - val_loss: 13.8104 - val_mae: 2.3357 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "42/42 [==============================] - 41s 976ms/step - loss: 35.4767 - mae: 2.7268 - val_loss: 13.7901 - val_mae: 2.3253 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "42/42 [==============================] - 48s 1s/step - loss: 34.9361 - mae: 2.6884 - val_loss: 13.3191 - val_mae: 2.2690 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "42/42 [==============================] - 48s 1s/step - loss: 34.8836 - mae: 2.6943 - val_loss: 13.3294 - val_mae: 2.2716 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "42/42 [==============================] - 39s 935ms/step - loss: 34.5950 - mae: 2.6749 - val_loss: 13.5933 - val_mae: 2.3561 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 34.4242 - mae: 2.6789 - val_loss: 13.0194 - val_mae: 2.2466 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "42/42 [==============================] - 40s 931ms/step - loss: 34.1179 - mae: 2.6545 - val_loss: 12.9887 - val_mae: 2.2558 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "42/42 [==============================] - 41s 968ms/step - loss: 33.8005 - mae: 2.6343 - val_loss: 12.8410 - val_mae: 2.2124 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "42/42 [==============================] - 42s 987ms/step - loss: 33.5626 - mae: 2.6346 - val_loss: 12.7040 - val_mae: 2.2301 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 33.5643 - mae: 2.6460 - val_loss: 12.6404 - val_mae: 2.2001 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 33.0542 - mae: 2.6058 - val_loss: 12.5220 - val_mae: 2.1980 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 32.9452 - mae: 2.6037 - val_loss: 12.6967 - val_mae: 2.2169 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "42/42 [==============================] - 39s 934ms/step - loss: 32.6169 - mae: 2.5880 - val_loss: 12.3824 - val_mae: 2.1816 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "42/42 [==============================] - 46s 1s/step - loss: 32.6578 - mae: 2.6121 - val_loss: 12.2059 - val_mae: 2.1624 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 32.1726 - mae: 2.5680 - val_loss: 12.0816 - val_mae: 2.1496 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 32.1357 - mae: 2.5718 - val_loss: 11.9957 - val_mae: 2.1390 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 32.0291 - mae: 2.5782 - val_loss: 12.5621 - val_mae: 2.2090 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "42/42 [==============================] - 41s 976ms/step - loss: 31.8988 - mae: 2.5669 - val_loss: 11.9238 - val_mae: 2.1335 - lr: 1.0000e-04\n",
      "Epoch 150/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 43s 1s/step - loss: 31.8746 - mae: 2.5975 - val_loss: 12.1159 - val_mae: 2.1655 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 31.2692 - mae: 2.5455 - val_loss: 11.7203 - val_mae: 2.1328 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 30.9260 - mae: 2.5087 - val_loss: 11.5809 - val_mae: 2.1067 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "42/42 [==============================] - 38s 917ms/step - loss: 30.6954 - mae: 2.5109 - val_loss: 11.5030 - val_mae: 2.0959 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "42/42 [==============================] - 46s 1s/step - loss: 30.9298 - mae: 2.5580 - val_loss: 11.3920 - val_mae: 2.0863 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "42/42 [==============================] - 41s 975ms/step - loss: 30.3455 - mae: 2.4943 - val_loss: 11.4019 - val_mae: 2.0875 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "42/42 [==============================] - 39s 926ms/step - loss: 30.1533 - mae: 2.4829 - val_loss: 11.4175 - val_mae: 2.0817 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "42/42 [==============================] - 41s 976ms/step - loss: 29.9339 - mae: 2.4722 - val_loss: 11.1805 - val_mae: 2.0677 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "42/42 [==============================] - 41s 981ms/step - loss: 29.7843 - mae: 2.4706 - val_loss: 11.2622 - val_mae: 2.0866 - lr: 1.0000e-04\n",
      "Epoch 159/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 29.9591 - mae: 2.4916 - val_loss: 11.0460 - val_mae: 2.0475 - lr: 1.0000e-04\n",
      "Epoch 160/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 29.5728 - mae: 2.4782 - val_loss: 11.0839 - val_mae: 2.0748 - lr: 1.0000e-04\n",
      "Epoch 161/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 29.4771 - mae: 2.4978 - val_loss: 11.0185 - val_mae: 2.0592 - lr: 1.0000e-04\n",
      "Epoch 162/300\n",
      "42/42 [==============================] - 38s 904ms/step - loss: 29.1763 - mae: 2.4718 - val_loss: 10.8654 - val_mae: 2.0448 - lr: 1.0000e-04\n",
      "Epoch 163/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 29.2934 - mae: 2.4819 - val_loss: 11.0877 - val_mae: 2.1079 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 28.9279 - mae: 2.4660 - val_loss: 10.8192 - val_mae: 2.0352 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "42/42 [==============================] - 40s 958ms/step - loss: 28.4786 - mae: 2.4197 - val_loss: 10.8077 - val_mae: 2.0428 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "42/42 [==============================] - 42s 996ms/step - loss: 28.2340 - mae: 2.4098 - val_loss: 10.7264 - val_mae: 2.0407 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "42/42 [==============================] - 40s 959ms/step - loss: 28.1506 - mae: 2.4155 - val_loss: 10.7328 - val_mae: 2.0345 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 28.0823 - mae: 2.4203 - val_loss: 10.5996 - val_mae: 2.0124 - lr: 1.0000e-04\n",
      "Epoch 169/300\n",
      "42/42 [==============================] - 42s 986ms/step - loss: 27.9083 - mae: 2.4062 - val_loss: 10.9996 - val_mae: 2.1083 - lr: 1.0000e-04\n",
      "Epoch 170/300\n",
      "42/42 [==============================] - 41s 982ms/step - loss: 28.0323 - mae: 2.4424 - val_loss: 10.8841 - val_mae: 2.0946 - lr: 1.0000e-04\n",
      "Epoch 171/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 27.6183 - mae: 2.3996 - val_loss: 10.4676 - val_mae: 1.9953 - lr: 1.0000e-04\n",
      "Epoch 172/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 27.4355 - mae: 2.3926 - val_loss: 10.7049 - val_mae: 2.0489 - lr: 1.0000e-04\n",
      "Epoch 173/300\n",
      "42/42 [==============================] - 42s 993ms/step - loss: 27.4317 - mae: 2.4012 - val_loss: 10.5425 - val_mae: 2.0069 - lr: 1.0000e-04\n",
      "Epoch 174/300\n",
      "42/42 [==============================] - 41s 980ms/step - loss: 27.0979 - mae: 2.3776 - val_loss: 10.3622 - val_mae: 1.9862 - lr: 1.0000e-04\n",
      "Epoch 175/300\n",
      "42/42 [==============================] - 41s 969ms/step - loss: 27.0548 - mae: 2.3841 - val_loss: 10.4118 - val_mae: 1.9948 - lr: 1.0000e-04\n",
      "Epoch 176/300\n",
      "42/42 [==============================] - 41s 979ms/step - loss: 26.8514 - mae: 2.3932 - val_loss: 10.4188 - val_mae: 1.9926 - lr: 1.0000e-04\n",
      "Epoch 177/300\n",
      "42/42 [==============================] - 41s 984ms/step - loss: 26.9197 - mae: 2.4040 - val_loss: 10.2637 - val_mae: 1.9776 - lr: 1.0000e-04\n",
      "Epoch 178/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 26.3835 - mae: 2.3554 - val_loss: 10.2628 - val_mae: 1.9912 - lr: 1.0000e-04\n",
      "Epoch 179/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 26.3727 - mae: 2.3621 - val_loss: 10.2886 - val_mae: 1.9948 - lr: 1.0000e-04\n",
      "Epoch 180/300\n",
      "42/42 [==============================] - 38s 913ms/step - loss: 26.0589 - mae: 2.3511 - val_loss: 10.1362 - val_mae: 1.9763 - lr: 1.0000e-04\n",
      "Epoch 181/300\n",
      "42/42 [==============================] - 40s 954ms/step - loss: 25.9970 - mae: 2.3564 - val_loss: 10.0649 - val_mae: 1.9577 - lr: 1.0000e-04\n",
      "Epoch 182/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 25.7403 - mae: 2.3367 - val_loss: 10.1598 - val_mae: 1.9768 - lr: 1.0000e-04\n",
      "Epoch 183/300\n",
      "42/42 [==============================] - 40s 951ms/step - loss: 25.8380 - mae: 2.3557 - val_loss: 10.0073 - val_mae: 1.9611 - lr: 1.0000e-04\n",
      "Epoch 184/300\n",
      "42/42 [==============================] - 41s 972ms/step - loss: 25.5853 - mae: 2.3403 - val_loss: 10.0842 - val_mae: 1.9885 - lr: 1.0000e-04\n",
      "Epoch 185/300\n",
      "42/42 [==============================] - 41s 973ms/step - loss: 25.6274 - mae: 2.3802 - val_loss: 10.4371 - val_mae: 2.0460 - lr: 1.0000e-04\n",
      "Epoch 186/300\n",
      "42/42 [==============================] - 41s 987ms/step - loss: 25.5497 - mae: 2.3873 - val_loss: 10.4146 - val_mae: 2.0835 - lr: 1.0000e-04\n",
      "Epoch 187/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 25.1801 - mae: 2.3330 - val_loss: 9.9436 - val_mae: 1.9530 - lr: 1.0000e-04\n",
      "Epoch 188/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 24.8889 - mae: 2.3103 - val_loss: 9.9532 - val_mae: 1.9898 - lr: 1.0000e-04\n",
      "Epoch 189/300\n",
      "42/42 [==============================] - 38s 915ms/step - loss: 24.7916 - mae: 2.3120 - val_loss: 10.0610 - val_mae: 2.0196 - lr: 1.0000e-04\n",
      "Epoch 190/300\n",
      "42/42 [==============================] - 41s 974ms/step - loss: 24.6190 - mae: 2.3034 - val_loss: 9.7715 - val_mae: 1.9416 - lr: 1.0000e-04\n",
      "Epoch 191/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 24.5644 - mae: 2.3041 - val_loss: 9.8859 - val_mae: 1.9374 - lr: 1.0000e-04\n",
      "Epoch 192/300\n",
      "42/42 [==============================] - 38s 916ms/step - loss: 25.1739 - mae: 2.3585 - val_loss: 9.8164 - val_mae: 1.9417 - lr: 1.0000e-04\n",
      "Epoch 193/300\n",
      "42/42 [==============================] - 42s 997ms/step - loss: 24.3562 - mae: 2.3070 - val_loss: 9.7893 - val_mae: 1.9457 - lr: 1.0000e-04\n",
      "Epoch 194/300\n",
      "42/42 [==============================] - 40s 954ms/step - loss: 24.3007 - mae: 2.3057 - val_loss: 9.7884 - val_mae: 1.9235 - lr: 1.0000e-04\n",
      "Epoch 195/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 24.0632 - mae: 2.2879 - val_loss: 9.8143 - val_mae: 1.9516 - lr: 1.0000e-04\n",
      "Epoch 196/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 24.0726 - mae: 2.3111 - val_loss: 9.9647 - val_mae: 2.0172 - lr: 1.0000e-04\n",
      "Epoch 197/300\n",
      "42/42 [==============================] - 40s 954ms/step - loss: 23.8622 - mae: 2.2941 - val_loss: 9.5255 - val_mae: 1.9076 - lr: 1.0000e-04\n",
      "Epoch 198/300\n",
      "42/42 [==============================] - 40s 947ms/step - loss: 23.7849 - mae: 2.2827 - val_loss: 9.5058 - val_mae: 1.9123 - lr: 1.0000e-04\n",
      "Epoch 199/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 23.6719 - mae: 2.2877 - val_loss: 9.5554 - val_mae: 1.9225 - lr: 1.0000e-04\n",
      "Epoch 200/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 23.4949 - mae: 2.2737 - val_loss: 9.4842 - val_mae: 1.9061 - lr: 1.0000e-04\n",
      "Epoch 201/300\n",
      "42/42 [==============================] - 39s 935ms/step - loss: 23.2458 - mae: 2.2545 - val_loss: 9.3984 - val_mae: 1.8947 - lr: 1.0000e-04\n",
      "Epoch 202/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 23.0943 - mae: 2.2558 - val_loss: 9.4106 - val_mae: 1.9094 - lr: 1.0000e-04\n",
      "Epoch 203/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 40s 965ms/step - loss: 23.0255 - mae: 2.2561 - val_loss: 9.4653 - val_mae: 1.8990 - lr: 1.0000e-04\n",
      "Epoch 204/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 23.0792 - mae: 2.2699 - val_loss: 9.2699 - val_mae: 1.8805 - lr: 1.0000e-04\n",
      "Epoch 205/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 22.8703 - mae: 2.2519 - val_loss: 9.5952 - val_mae: 1.9128 - lr: 1.0000e-04\n",
      "Epoch 206/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 22.7281 - mae: 2.2407 - val_loss: 9.3176 - val_mae: 1.8979 - lr: 1.0000e-04\n",
      "Epoch 207/300\n",
      "42/42 [==============================] - 39s 920ms/step - loss: 22.5315 - mae: 2.2385 - val_loss: 9.3412 - val_mae: 1.8974 - lr: 1.0000e-04\n",
      "Epoch 208/300\n",
      "42/42 [==============================] - 40s 946ms/step - loss: 22.5576 - mae: 2.2536 - val_loss: 9.6841 - val_mae: 1.9986 - lr: 1.0000e-04\n",
      "Epoch 209/300\n",
      "42/42 [==============================] - 40s 954ms/step - loss: 22.7798 - mae: 2.2919 - val_loss: 9.6773 - val_mae: 1.9116 - lr: 1.0000e-04\n",
      "Epoch 210/300\n",
      "42/42 [==============================] - 40s 954ms/step - loss: 22.1569 - mae: 2.2314 - val_loss: 9.2334 - val_mae: 1.8794 - lr: 1.0000e-04\n",
      "Epoch 211/300\n",
      "42/42 [==============================] - 40s 956ms/step - loss: 22.1971 - mae: 2.2463 - val_loss: 9.1092 - val_mae: 1.8676 - lr: 1.0000e-04\n",
      "Epoch 212/300\n",
      "42/42 [==============================] - 41s 974ms/step - loss: 21.9160 - mae: 2.2208 - val_loss: 9.1168 - val_mae: 1.8766 - lr: 1.0000e-04\n",
      "Epoch 213/300\n",
      "42/42 [==============================] - 40s 963ms/step - loss: 22.1731 - mae: 2.2640 - val_loss: 9.1749 - val_mae: 1.8790 - lr: 1.0000e-04\n",
      "Epoch 214/300\n",
      "42/42 [==============================] - 51s 1s/step - loss: 21.8538 - mae: 2.2167 - val_loss: 9.3230 - val_mae: 1.9150 - lr: 1.0000e-04\n",
      "Epoch 215/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 21.8845 - mae: 2.2332 - val_loss: 9.0214 - val_mae: 1.8672 - lr: 1.0000e-04\n",
      "Epoch 216/300\n",
      "42/42 [==============================] - 37s 891ms/step - loss: 21.6191 - mae: 2.2172 - val_loss: 9.4989 - val_mae: 2.0066 - lr: 1.0000e-04\n",
      "Epoch 217/300\n",
      "42/42 [==============================] - 38s 909ms/step - loss: 21.6946 - mae: 2.2431 - val_loss: 9.0701 - val_mae: 1.8870 - lr: 1.0000e-04\n",
      "Epoch 218/300\n",
      "42/42 [==============================] - 51s 1s/step - loss: 21.2779 - mae: 2.2056 - val_loss: 9.1330 - val_mae: 1.8826 - lr: 1.0000e-04\n",
      "Epoch 219/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 21.3080 - mae: 2.2133 - val_loss: 8.9220 - val_mae: 1.8481 - lr: 1.0000e-04\n",
      "Epoch 220/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 21.8323 - mae: 2.2444 - val_loss: 9.2877 - val_mae: 1.9661 - lr: 1.0000e-04\n",
      "Epoch 221/300\n",
      "42/42 [==============================] - 47s 1s/step - loss: 21.2276 - mae: 2.2240 - val_loss: 8.8058 - val_mae: 1.8356 - lr: 1.0000e-04\n",
      "Epoch 222/300\n",
      "42/42 [==============================] - 50s 1s/step - loss: 21.0032 - mae: 2.2114 - val_loss: 8.9929 - val_mae: 1.8650 - lr: 1.0000e-04\n",
      "Epoch 223/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 20.7893 - mae: 2.1952 - val_loss: 9.0311 - val_mae: 1.8817 - lr: 1.0000e-04\n",
      "Epoch 224/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 20.8034 - mae: 2.2084 - val_loss: 8.9878 - val_mae: 1.8996 - lr: 1.0000e-04\n",
      "Epoch 225/300\n",
      "42/42 [==============================] - 46s 1s/step - loss: 20.8049 - mae: 2.2037 - val_loss: 9.3271 - val_mae: 1.9061 - lr: 1.0000e-04\n",
      "Epoch 226/300\n",
      "42/42 [==============================] - 42s 994ms/step - loss: 20.6503 - mae: 2.2083 - val_loss: 8.9028 - val_mae: 1.8682 - lr: 1.0000e-04\n",
      "Epoch 227/300\n",
      "42/42 [==============================] - 38s 918ms/step - loss: 21.1104 - mae: 2.2295 - val_loss: 8.7394 - val_mae: 1.8299 - lr: 1.0000e-04\n",
      "Epoch 228/300\n",
      "42/42 [==============================] - 39s 934ms/step - loss: 20.5283 - mae: 2.1950 - val_loss: 9.5619 - val_mae: 1.9848 - lr: 1.0000e-04\n",
      "Epoch 229/300\n",
      "42/42 [==============================] - 42s 999ms/step - loss: 20.8725 - mae: 2.2463 - val_loss: 8.7340 - val_mae: 1.8330 - lr: 1.0000e-04\n",
      "Epoch 230/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 20.2113 - mae: 2.1840 - val_loss: 9.5462 - val_mae: 2.0115 - lr: 1.0000e-04\n",
      "Epoch 231/300\n",
      "42/42 [==============================] - 41s 986ms/step - loss: 20.5710 - mae: 2.2032 - val_loss: 8.7746 - val_mae: 1.8594 - lr: 1.0000e-04\n",
      "Epoch 232/300\n",
      "42/42 [==============================] - 41s 977ms/step - loss: 20.1421 - mae: 2.1818 - val_loss: 8.9059 - val_mae: 1.8442 - lr: 1.0000e-04\n",
      "Epoch 233/300\n",
      "42/42 [==============================] - 41s 975ms/step - loss: 19.8136 - mae: 2.1530 - val_loss: 8.5687 - val_mae: 1.8084 - lr: 1.0000e-04\n",
      "Epoch 234/300\n",
      "42/42 [==============================] - 40s 949ms/step - loss: 19.9687 - mae: 2.1745 - val_loss: 8.6434 - val_mae: 1.8318 - lr: 1.0000e-04\n",
      "Epoch 235/300\n",
      "42/42 [==============================] - 38s 919ms/step - loss: 19.7037 - mae: 2.1557 - val_loss: 8.7257 - val_mae: 1.8573 - lr: 1.0000e-04\n",
      "Epoch 236/300\n",
      "42/42 [==============================] - 49s 1s/step - loss: 19.6231 - mae: 2.1579 - val_loss: 8.4387 - val_mae: 1.7965 - lr: 1.0000e-04\n",
      "Epoch 237/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 19.5807 - mae: 2.1600 - val_loss: 8.7249 - val_mae: 1.8511 - lr: 1.0000e-04\n",
      "Epoch 238/300\n",
      "42/42 [==============================] - 41s 982ms/step - loss: 19.5028 - mae: 2.1572 - val_loss: 8.8607 - val_mae: 1.8775 - lr: 1.0000e-04\n",
      "Epoch 239/300\n",
      "42/42 [==============================] - 36s 861ms/step - loss: 19.4100 - mae: 2.1512 - val_loss: 8.7734 - val_mae: 1.8349 - lr: 1.0000e-04\n",
      "Epoch 240/300\n",
      "42/42 [==============================] - 39s 918ms/step - loss: 19.3776 - mae: 2.1620 - val_loss: 8.4438 - val_mae: 1.8142 - lr: 1.0000e-04\n",
      "Epoch 241/300\n",
      "42/42 [==============================] - 40s 948ms/step - loss: 19.9090 - mae: 2.1885 - val_loss: 8.5572 - val_mae: 1.8436 - lr: 1.0000e-04\n",
      "Epoch 242/300\n",
      "42/42 [==============================] - 38s 900ms/step - loss: 19.9611 - mae: 2.1815 - val_loss: 8.5639 - val_mae: 1.8238 - lr: 1.0000e-04\n",
      "Epoch 243/300\n",
      "42/42 [==============================] - 40s 961ms/step - loss: 19.2738 - mae: 2.1565 - val_loss: 8.4606 - val_mae: 1.7953 - lr: 1.0000e-04\n",
      "Epoch 244/300\n",
      "42/42 [==============================] - 39s 922ms/step - loss: 19.1424 - mae: 2.1541 - val_loss: 8.4442 - val_mae: 1.8025 - lr: 1.0000e-04\n",
      "Epoch 245/300\n",
      "42/42 [==============================] - 39s 943ms/step - loss: 18.9815 - mae: 2.1404 - val_loss: 8.4766 - val_mae: 1.8025 - lr: 1.0000e-04\n",
      "Epoch 246/300\n",
      "42/42 [==============================] - 43s 1s/step - loss: 18.9713 - mae: 2.1364 - val_loss: 8.3585 - val_mae: 1.7864 - lr: 1.0000e-04\n",
      "Epoch 247/300\n",
      "42/42 [==============================] - 36s 866ms/step - loss: 19.2008 - mae: 2.1518 - val_loss: 8.8462 - val_mae: 1.9025 - lr: 1.0000e-04\n",
      "Epoch 248/300\n",
      "42/42 [==============================] - 36s 848ms/step - loss: 19.1520 - mae: 2.1672 - val_loss: 8.5281 - val_mae: 1.8120 - lr: 1.0000e-04\n",
      "Epoch 249/300\n",
      "42/42 [==============================] - 36s 864ms/step - loss: 19.6140 - mae: 2.2106 - val_loss: 8.7094 - val_mae: 1.8485 - lr: 1.0000e-04\n",
      "Epoch 250/300\n",
      "42/42 [==============================] - 35s 826ms/step - loss: 18.6573 - mae: 2.1455 - val_loss: 8.5925 - val_mae: 1.8551 - lr: 1.0000e-04\n",
      "Epoch 251/300\n",
      "42/42 [==============================] - 36s 856ms/step - loss: 18.5521 - mae: 2.1345 - val_loss: 8.8326 - val_mae: 1.9410 - lr: 1.0000e-04\n",
      "Epoch 252/300\n",
      "42/42 [==============================] - 36s 852ms/step - loss: 18.3662 - mae: 2.1242 - val_loss: 8.3786 - val_mae: 1.8051 - lr: 1.0000e-04\n",
      "Epoch 253/300\n",
      "42/42 [==============================] - 38s 907ms/step - loss: 18.4572 - mae: 2.1205 - val_loss: 8.4444 - val_mae: 1.7789 - lr: 1.0000e-04\n",
      "Epoch 254/300\n",
      "42/42 [==============================] - 45s 1s/step - loss: 18.3846 - mae: 2.1243 - val_loss: 8.5779 - val_mae: 1.8643 - lr: 1.0000e-04\n",
      "Epoch 255/300\n",
      "42/42 [==============================] - 39s 938ms/step - loss: 18.3642 - mae: 2.1373 - val_loss: 8.4276 - val_mae: 1.8094 - lr: 1.0000e-04\n",
      "Epoch 256/300\n",
      "42/42 [==============================] - 36s 860ms/step - loss: 18.5528 - mae: 2.1541 - val_loss: 8.3353 - val_mae: 1.7868 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/300\n",
      "42/42 [==============================] - 38s 899ms/step - loss: 18.2033 - mae: 2.1327 - val_loss: 8.2115 - val_mae: 1.7701 - lr: 1.0000e-04\n",
      "Epoch 258/300\n",
      "42/42 [==============================] - 39s 934ms/step - loss: 18.0395 - mae: 2.1001 - val_loss: 8.1303 - val_mae: 1.7585 - lr: 1.0000e-04\n",
      "Epoch 259/300\n",
      "42/42 [==============================] - 40s 954ms/step - loss: 18.0946 - mae: 2.1299 - val_loss: 8.6559 - val_mae: 1.9249 - lr: 1.0000e-04\n",
      "Epoch 260/300\n",
      "42/42 [==============================] - 35s 846ms/step - loss: 17.9496 - mae: 2.1193 - val_loss: 8.6850 - val_mae: 1.8594 - lr: 1.0000e-04\n",
      "Epoch 261/300\n",
      "42/42 [==============================] - 47s 1s/step - loss: 18.5294 - mae: 2.1400 - val_loss: 8.0623 - val_mae: 1.7617 - lr: 1.0000e-04\n",
      "Epoch 262/300\n",
      "42/42 [==============================] - 44s 1s/step - loss: 18.2876 - mae: 2.1392 - val_loss: 8.6006 - val_mae: 1.8669 - lr: 1.0000e-04\n",
      "Epoch 263/300\n",
      "42/42 [==============================] - 41s 986ms/step - loss: 17.7809 - mae: 2.0987 - val_loss: 8.0364 - val_mae: 1.7431 - lr: 1.0000e-04\n",
      "Epoch 264/300\n",
      "42/42 [==============================] - 41s 970ms/step - loss: 17.5125 - mae: 2.0795 - val_loss: 8.4534 - val_mae: 1.8476 - lr: 1.0000e-04\n",
      "Epoch 265/300\n",
      "42/42 [==============================] - 36s 857ms/step - loss: 18.0464 - mae: 2.1442 - val_loss: 8.3003 - val_mae: 1.7653 - lr: 1.0000e-04\n",
      "Epoch 266/300\n",
      "42/42 [==============================] - 39s 940ms/step - loss: 17.5421 - mae: 2.0949 - val_loss: 8.0262 - val_mae: 1.7478 - lr: 1.0000e-04\n",
      "Epoch 267/300\n",
      "42/42 [==============================] - 38s 896ms/step - loss: 17.3403 - mae: 2.0731 - val_loss: 7.9929 - val_mae: 1.7399 - lr: 1.0000e-04\n",
      "Epoch 268/300\n",
      "42/42 [==============================] - 40s 949ms/step - loss: 17.3467 - mae: 2.0761 - val_loss: 7.9716 - val_mae: 1.7519 - lr: 1.0000e-04\n",
      "Epoch 269/300\n",
      "42/42 [==============================] - 39s 920ms/step - loss: 17.2496 - mae: 2.0691 - val_loss: 8.0791 - val_mae: 1.7464 - lr: 1.0000e-04\n",
      "Epoch 270/300\n",
      "42/42 [==============================] - 37s 876ms/step - loss: 17.2951 - mae: 2.0751 - val_loss: 7.8949 - val_mae: 1.7311 - lr: 1.0000e-04\n",
      "Epoch 271/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 17.2314 - mae: 2.0808 - val_loss: 8.1975 - val_mae: 1.8139 - lr: 1.0000e-04\n",
      "Epoch 272/300\n",
      "42/42 [==============================] - 38s 917ms/step - loss: 17.1951 - mae: 2.0865 - val_loss: 7.9495 - val_mae: 1.7593 - lr: 1.0000e-04\n",
      "Epoch 273/300\n",
      "42/42 [==============================] - 35s 824ms/step - loss: 16.9992 - mae: 2.0748 - val_loss: 7.8866 - val_mae: 1.7442 - lr: 1.0000e-04\n",
      "Epoch 274/300\n",
      "42/42 [==============================] - 36s 848ms/step - loss: 17.0023 - mae: 2.0783 - val_loss: 7.9156 - val_mae: 1.7342 - lr: 1.0000e-04\n",
      "Epoch 275/300\n",
      "42/42 [==============================] - 38s 908ms/step - loss: 16.9832 - mae: 2.0778 - val_loss: 7.9110 - val_mae: 1.7321 - lr: 1.0000e-04\n",
      "Epoch 276/300\n",
      "42/42 [==============================] - 37s 882ms/step - loss: 17.1454 - mae: 2.0761 - val_loss: 7.9782 - val_mae: 1.7628 - lr: 1.0000e-04\n",
      "Epoch 277/300\n",
      "42/42 [==============================] - 39s 939ms/step - loss: 16.7673 - mae: 2.0591 - val_loss: 7.8939 - val_mae: 1.7241 - lr: 1.0000e-04\n",
      "Epoch 278/300\n",
      "42/42 [==============================] - 38s 905ms/step - loss: 16.7445 - mae: 2.0557 - val_loss: 7.9719 - val_mae: 1.7546 - lr: 1.0000e-04\n",
      "Epoch 279/300\n",
      "42/42 [==============================] - 38s 910ms/step - loss: 16.8222 - mae: 2.0725 - val_loss: 7.8339 - val_mae: 1.7213 - lr: 1.0000e-04\n",
      "Epoch 280/300\n",
      "42/42 [==============================] - 38s 917ms/step - loss: 16.7296 - mae: 2.0651 - val_loss: 7.7933 - val_mae: 1.7194 - lr: 1.0000e-04\n",
      "Epoch 281/300\n",
      "42/42 [==============================] - 41s 970ms/step - loss: 16.8468 - mae: 2.0952 - val_loss: 8.6159 - val_mae: 1.9145 - lr: 1.0000e-04\n",
      "Epoch 282/300\n",
      "42/42 [==============================] - 36s 861ms/step - loss: 16.9738 - mae: 2.1413 - val_loss: 7.7024 - val_mae: 1.7068 - lr: 1.0000e-04\n",
      "Epoch 283/300\n",
      "42/42 [==============================] - 34s 820ms/step - loss: 16.3984 - mae: 2.0364 - val_loss: 7.7115 - val_mae: 1.7160 - lr: 1.0000e-04\n",
      "Epoch 284/300\n",
      "42/42 [==============================] - 36s 863ms/step - loss: 16.4222 - mae: 2.0356 - val_loss: 7.7980 - val_mae: 1.7249 - lr: 1.0000e-04\n",
      "Epoch 285/300\n",
      "42/42 [==============================] - 36s 860ms/step - loss: 16.4027 - mae: 2.0424 - val_loss: 7.8300 - val_mae: 1.7277 - lr: 1.0000e-04\n",
      "Epoch 286/300\n",
      "42/42 [==============================] - 38s 901ms/step - loss: 16.2871 - mae: 2.0459 - val_loss: 7.8621 - val_mae: 1.7640 - lr: 1.0000e-04\n",
      "Epoch 287/300\n",
      "42/42 [==============================] - 40s 948ms/step - loss: 16.5881 - mae: 2.0733 - val_loss: 8.0583 - val_mae: 1.7819 - lr: 1.0000e-04\n",
      "Epoch 288/300\n",
      "42/42 [==============================] - 38s 894ms/step - loss: 16.4595 - mae: 2.0636 - val_loss: 7.8266 - val_mae: 1.7342 - lr: 1.0000e-04\n",
      "Epoch 289/300\n",
      "42/42 [==============================] - 36s 868ms/step - loss: 17.8313 - mae: 2.1394 - val_loss: 7.7493 - val_mae: 1.7222 - lr: 1.0000e-04\n",
      "Epoch 290/300\n",
      "42/42 [==============================] - 41s 983ms/step - loss: 16.3392 - mae: 2.0471 - val_loss: 7.6910 - val_mae: 1.7091 - lr: 1.0000e-04\n",
      "Epoch 291/300\n",
      "42/42 [==============================] - 42s 1s/step - loss: 16.0359 - mae: 2.0253 - val_loss: 7.7617 - val_mae: 1.7115 - lr: 1.0000e-04\n",
      "Epoch 292/300\n",
      "42/42 [==============================] - 35s 839ms/step - loss: 16.3986 - mae: 2.0477 - val_loss: 7.6639 - val_mae: 1.7055 - lr: 1.0000e-04\n",
      "Epoch 293/300\n",
      "42/42 [==============================] - 36s 850ms/step - loss: 15.9383 - mae: 2.0238 - val_loss: 8.0556 - val_mae: 1.7965 - lr: 1.0000e-04\n",
      "Epoch 294/300\n",
      "42/42 [==============================] - 36s 856ms/step - loss: 15.9470 - mae: 2.0294 - val_loss: 7.8482 - val_mae: 1.7860 - lr: 1.0000e-04\n",
      "Epoch 295/300\n",
      "42/42 [==============================] - 35s 845ms/step - loss: 15.8000 - mae: 2.0207 - val_loss: 7.6674 - val_mae: 1.7074 - lr: 1.0000e-04\n",
      "Epoch 296/300\n",
      "42/42 [==============================] - 36s 851ms/step - loss: 15.7337 - mae: 2.0112 - val_loss: 7.5586 - val_mae: 1.6850 - lr: 1.0000e-04\n",
      "Epoch 297/300\n",
      "42/42 [==============================] - 36s 868ms/step - loss: 15.6627 - mae: 2.0017 - val_loss: 7.4876 - val_mae: 1.6873 - lr: 1.0000e-04\n",
      "Epoch 298/300\n",
      "42/42 [==============================] - 37s 879ms/step - loss: 15.9028 - mae: 2.0333 - val_loss: 7.4750 - val_mae: 1.6797 - lr: 1.0000e-04\n",
      "Epoch 299/300\n",
      "42/42 [==============================] - 38s 913ms/step - loss: 15.5475 - mae: 1.9957 - val_loss: 7.5636 - val_mae: 1.6918 - lr: 1.0000e-04\n",
      "Epoch 300/300\n",
      "42/42 [==============================] - 39s 928ms/step - loss: 15.8354 - mae: 2.0413 - val_loss: 7.7896 - val_mae: 1.7227 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# from sst and rainfall ---> rainfall\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Conv2D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Lambda\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2DTranspose, TimeDistributed\n",
    "from keras import callbacks\n",
    "import xarray as xr\n",
    "\n",
    "# Define global variables\n",
    "lead_time = 0\n",
    "save_path = \"/home/cccr/roxy/matin/MTech_project/model/Conv-LSTM/7in1out/\"\n",
    "model_path = save_path + \"ConvLstm_sst0rf_rf_0SI.h5\"\n",
    "add_data = \"/home/cccr/roxy/matin/MTech_project/data/\"\n",
    "\n",
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"1998-01-01\",\"2016-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_0lag.nc\",\"SIFilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n",
    "# Perform additional processing or modeling steps as needed\n",
    "seq = tf.keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "seq.add(ConvLSTM2D(filters=4, kernel_size=(3,3), padding='same', input_shape=(7,25,39,2), return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=False, data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=15, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "# Create the query, value, and key inputs\n",
    "query_input = Input(shape=(25,39,2), name='query_input')\n",
    "value_input = Input(shape=(25,39,2), name='value_input')\n",
    "key_input = Input(shape=(25,39,2), name='key_input')\n",
    "\n",
    "# Pass the inputs to the Attention layer\n",
    "attention_output = Attention(name='attention')([query_input, value_input, key_input])\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=10**-4)\n",
    "seq.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae'])\n",
    "\n",
    "print(seq.summary())\n",
    "\n",
    "# Define the number of training epochs\n",
    "n_epochs = 300\n",
    "logdir = save_path\n",
    "# Define the TensorBoard callback for monitoring training progress\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=10, batch_size=10, write_graph=True)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback for reducing the learning rate when the model plateaus\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=10**-20)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation mean absolute error\n",
    "checkpoint_callback_mae = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_mae',\n",
    "                                                             mode='min', save_best_only=True)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation loss\n",
    "checkpoint_callback_loss = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss',\n",
    "                                                              mode='min', save_best_only=True)\n",
    "\n",
    "# Define the EarlyStopping callback for stopping training when the model stops improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "# Define the TerminateOnNaN callback for terminating the training process if there is NaN value in any of the output\n",
    "terminate_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "# Train the model using the defined callbacks\n",
    "history = seq.fit(x, y, epochs=n_epochs, validation_split=0.1,\n",
    "                  batch_size=50, callbacks=[early_stop_callback,\n",
    "                  checkpoint_callback_mae, checkpoint_callback_loss,\n",
    "                  tb_callback, lr_callback, terminate_callback])\n",
    "#Save the trained model\n",
    "seq.save(model_path)\n",
    "\n",
    "#Save the training history\n",
    "\n",
    "np.save(f'{save_path}ConvLstm_sst0rf_rf_0SI.npy', history.history)               \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68062afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (237, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (237, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (237, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (237, 1, 25, 39)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"2017-01-01\",\"2018-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_0lag.nc\",\"SIFilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92700252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 99ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.87096774],\n",
       "       [0.87096774, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "pred = model.predict(x)\n",
    "\n",
    "actual = y.flatten()\n",
    "prediction = pred.flatten()\n",
    "corr = np.corrcoef(actual,prediction)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e8a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1606c222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
