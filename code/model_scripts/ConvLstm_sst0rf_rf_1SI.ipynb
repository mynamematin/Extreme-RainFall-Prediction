{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96757df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (2310, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (2310, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (2310, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (2310, 1, 25, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 15:31:54.035393: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 7, 25, 39, 4)      880       \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 7, 25, 39, 8)      3488      \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 7, 25, 39, 8)      4640      \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 7, 25, 39, 16)     13888     \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 25, 39, 16)        18496     \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 25, 39, 15)        2175      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 39, 1)         136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,703\n",
      "Trainable params: 43,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/200\n",
      "42/42 [==============================] - 139s 3s/step - loss: 1215.9916 - mae: 32.1785 - val_loss: 1094.2041 - val_mae: 31.5920 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 232s 6s/step - loss: 1156.8046 - mae: 31.2452 - val_loss: 887.5377 - val_mae: 28.0897 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 757.6884 - mae: 23.8991 - val_loss: 492.8546 - val_mae: 19.8337 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 507.5169 - mae: 18.0536 - val_loss: 326.0862 - val_mae: 15.0997 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 280s 7s/step - loss: 365.7566 - mae: 13.7594 - val_loss: 218.3749 - val_mae: 11.0947 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 278.3594 - mae: 10.5018 - val_loss: 158.9431 - val_mae: 8.2181 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 233.5211 - mae: 8.7310 - val_loss: 134.1240 - val_mae: 7.0611 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 215.7190 - mae: 8.2717 - val_loss: 126.4215 - val_mae: 7.0048 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 210.2590 - mae: 8.3261 - val_loss: 124.9470 - val_mae: 7.1963 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 208.9843 - mae: 8.4498 - val_loss: 124.9122 - val_mae: 7.3363 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 208.6948 - mae: 8.5063 - val_loss: 124.9090 - val_mae: 7.3660 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 208.5998 - mae: 8.5281 - val_loss: 124.9472 - val_mae: 7.4045 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 208.5301 - mae: 8.5455 - val_loss: 124.8500 - val_mae: 7.3925 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 208.4663 - mae: 8.5411 - val_loss: 124.7980 - val_mae: 7.3965 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 208.3917 - mae: 8.5420 - val_loss: 124.7622 - val_mae: 7.4059 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 208.3469 - mae: 8.5407 - val_loss: 124.7502 - val_mae: 7.4223 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 208.2617 - mae: 8.5522 - val_loss: 124.6101 - val_mae: 7.3989 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 208.1934 - mae: 8.5317 - val_loss: 124.5329 - val_mae: 7.3958 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 208.1151 - mae: 8.5431 - val_loss: 124.4955 - val_mae: 7.4054 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 208.0420 - mae: 8.5349 - val_loss: 124.3388 - val_mae: 7.3768 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 280s 7s/step - loss: 207.9664 - mae: 8.5221 - val_loss: 124.2919 - val_mae: 7.3852 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 207.8983 - mae: 8.5320 - val_loss: 124.2173 - val_mae: 7.3845 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 207.8232 - mae: 8.5013 - val_loss: 124.1067 - val_mae: 7.3723 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 207.7421 - mae: 8.5394 - val_loss: 124.1105 - val_mae: 7.3976 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 207.6669 - mae: 8.5288 - val_loss: 123.8768 - val_mae: 7.3449 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 207.5710 - mae: 8.5030 - val_loss: 123.8630 - val_mae: 7.3681 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 207.4925 - mae: 8.5337 - val_loss: 123.8494 - val_mae: 7.3890 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 207.3960 - mae: 8.4845 - val_loss: 123.6277 - val_mae: 7.3427 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 280s 7s/step - loss: 207.3010 - mae: 8.4916 - val_loss: 123.6166 - val_mae: 7.3672 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 207.2091 - mae: 8.5021 - val_loss: 123.5580 - val_mae: 7.3752 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 273s 7s/step - loss: 207.1133 - mae: 8.5200 - val_loss: 123.3934 - val_mae: 7.3502 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 281s 7s/step - loss: 207.0237 - mae: 8.4905 - val_loss: 123.2898 - val_mae: 7.3451 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 284s 7s/step - loss: 206.9217 - mae: 8.4976 - val_loss: 123.1912 - val_mae: 7.3421 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 206.8528 - mae: 8.5121 - val_loss: 123.0672 - val_mae: 7.3310 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 206.7494 - mae: 8.4427 - val_loss: 122.9240 - val_mae: 7.3129 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 206.6449 - mae: 8.5042 - val_loss: 122.9239 - val_mae: 7.3451 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 206.5471 - mae: 8.4728 - val_loss: 122.8095 - val_mae: 7.3390 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 206.4201 - mae: 8.4808 - val_loss: 122.6681 - val_mae: 7.3245 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 206.3170 - mae: 8.4592 - val_loss: 122.5793 - val_mae: 7.3275 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 206.1869 - mae: 8.4602 - val_loss: 122.4224 - val_mae: 7.3088 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 274s 6s/step - loss: 206.0769 - mae: 8.4636 - val_loss: 122.3235 - val_mae: 7.3099 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 205.9906 - mae: 8.4409 - val_loss: 122.2494 - val_mae: 7.3189 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 273s 6s/step - loss: 205.8491 - mae: 8.4741 - val_loss: 122.1155 - val_mae: 7.3097 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 205.7426 - mae: 8.4568 - val_loss: 121.9999 - val_mae: 7.3067 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 205.6353 - mae: 8.4281 - val_loss: 121.8866 - val_mae: 7.3047 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 205.5117 - mae: 8.4346 - val_loss: 121.7334 - val_mae: 7.2906 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 205.3977 - mae: 8.4644 - val_loss: 121.6110 - val_mae: 7.2868 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 205.2660 - mae: 8.4028 - val_loss: 121.4254 - val_mae: 7.2621 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 205.1235 - mae: 8.4452 - val_loss: 121.4718 - val_mae: 7.3128 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 205.0034 - mae: 8.4127 - val_loss: 121.2529 - val_mae: 7.2828 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 204.8508 - mae: 8.4337 - val_loss: 121.0280 - val_mae: 7.2479 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 204.7652 - mae: 8.3873 - val_loss: 121.0444 - val_mae: 7.2903 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 204.5899 - mae: 8.4117 - val_loss: 120.8080 - val_mae: 7.2552 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 284s 7s/step - loss: 204.4986 - mae: 8.4181 - val_loss: 120.6190 - val_mae: 7.2332 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 204.3589 - mae: 8.3978 - val_loss: 120.5011 - val_mae: 7.2355 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 204.2759 - mae: 8.3980 - val_loss: 120.3672 - val_mae: 7.2327 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 204.0742 - mae: 8.3591 - val_loss: 120.2496 - val_mae: 7.2354 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 203.9138 - mae: 8.3775 - val_loss: 120.1916 - val_mae: 7.2557 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 273s 7s/step - loss: 203.7428 - mae: 8.3834 - val_loss: 119.9026 - val_mae: 7.2058 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 203.6105 - mae: 8.3355 - val_loss: 119.8811 - val_mae: 7.2396 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 203.4663 - mae: 8.3865 - val_loss: 119.7834 - val_mae: 7.2484 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 203.2929 - mae: 8.3781 - val_loss: 119.4158 - val_mae: 7.1752 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 203.1561 - mae: 8.3452 - val_loss: 119.3089 - val_mae: 7.1855 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 203.0993 - mae: 8.3241 - val_loss: 119.4635 - val_mae: 7.2660 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 202.8366 - mae: 8.4074 - val_loss: 118.7947 - val_mae: 7.0834 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 202.8407 - mae: 8.2598 - val_loss: 119.1357 - val_mae: 7.2498 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 238s 6s/step - loss: 202.5453 - mae: 8.3461 - val_loss: 118.7347 - val_mae: 7.1777 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 202.3811 - mae: 8.3543 - val_loss: 118.4468 - val_mae: 7.1278 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 202.2349 - mae: 8.2989 - val_loss: 118.3712 - val_mae: 7.1517 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 202.0426 - mae: 8.2973 - val_loss: 118.1125 - val_mae: 7.1115 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 201.8941 - mae: 8.3104 - val_loss: 117.9616 - val_mae: 7.1103 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 201.7867 - mae: 8.2706 - val_loss: 117.9687 - val_mae: 7.1599 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 201.5543 - mae: 8.3072 - val_loss: 117.7020 - val_mae: 7.1230 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 281s 7s/step - loss: 201.4031 - mae: 8.2769 - val_loss: 117.3764 - val_mae: 7.0562 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 201.2405 - mae: 8.2731 - val_loss: 117.3077 - val_mae: 7.0907 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 201.0597 - mae: 8.2584 - val_loss: 117.1975 - val_mae: 7.1047 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 200.8896 - mae: 8.2528 - val_loss: 117.0686 - val_mae: 7.1112 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 200.6960 - mae: 8.2684 - val_loss: 116.8203 - val_mae: 7.0805 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 200.5429 - mae: 8.2521 - val_loss: 116.6292 - val_mae: 7.0685 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 200.3663 - mae: 8.2424 - val_loss: 116.5127 - val_mae: 7.0808 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 200.2434 - mae: 8.2315 - val_loss: 116.3663 - val_mae: 7.0830 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 200.0036 - mae: 8.2034 - val_loss: 116.1283 - val_mae: 7.0574 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 199.7915 - mae: 8.2150 - val_loss: 115.9780 - val_mae: 7.0591 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 199.6200 - mae: 8.2268 - val_loss: 115.6775 - val_mae: 7.0116 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 199.4325 - mae: 8.1905 - val_loss: 115.6005 - val_mae: 7.0403 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 199.2205 - mae: 8.1659 - val_loss: 115.4600 - val_mae: 7.0461 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 198.9881 - mae: 8.2195 - val_loss: 115.0567 - val_mae: 6.9790 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 198.4178 - mae: 8.1175 - val_loss: 114.5093 - val_mae: 6.9353 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 197.3968 - mae: 8.1084 - val_loss: 113.8703 - val_mae: 6.8413 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 195.3305 - mae: 7.9831 - val_loss: 113.9528 - val_mae: 6.9190 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 193.1594 - mae: 7.9090 - val_loss: 114.2925 - val_mae: 7.0397 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 190.8545 - mae: 7.8659 - val_loss: 115.3186 - val_mae: 6.9048 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 188.1920 - mae: 7.7473 - val_loss: 111.8246 - val_mae: 6.9440 - lr: 1.0000e-04\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 183.3396 - mae: 7.6285 - val_loss: 109.7455 - val_mae: 6.8620 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 179.5411 - mae: 7.5359 - val_loss: 106.4662 - val_mae: 6.6555 - lr: 1.0000e-04\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 176.4551 - mae: 7.4294 - val_loss: 105.5700 - val_mae: 6.8760 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 174.0915 - mae: 7.4175 - val_loss: 103.3489 - val_mae: 6.5339 - lr: 1.0000e-04\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 281s 7s/step - loss: 171.6030 - mae: 7.3213 - val_loss: 99.9988 - val_mae: 6.6327 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 277s 7s/step - loss: 168.0115 - mae: 7.2241 - val_loss: 97.2907 - val_mae: 6.4544 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 278s 7s/step - loss: 165.1608 - mae: 7.1507 - val_loss: 96.1284 - val_mae: 6.5002 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 161.6966 - mae: 7.0441 - val_loss: 92.9871 - val_mae: 6.2533 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 157.8902 - mae: 6.9534 - val_loss: 88.1494 - val_mae: 6.1615 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 149.3483 - mae: 6.7346 - val_loss: 80.2442 - val_mae: 6.0148 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 136.2596 - mae: 6.3171 - val_loss: 66.1275 - val_mae: 5.3353 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 119.7160 - mae: 5.6644 - val_loss: 57.0203 - val_mae: 4.9990 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 109.1231 - mae: 5.2366 - val_loss: 47.9589 - val_mae: 4.4875 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 102.3834 - mae: 4.9616 - val_loss: 44.4047 - val_mae: 4.2637 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 97.0543 - mae: 4.7595 - val_loss: 40.6783 - val_mae: 4.0496 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 92.5041 - mae: 4.5901 - val_loss: 37.9448 - val_mae: 3.8864 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 88.1329 - mae: 4.4225 - val_loss: 36.5480 - val_mae: 3.8126 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 84.6655 - mae: 4.2960 - val_loss: 34.8193 - val_mae: 3.7168 - lr: 1.0000e-04\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 81.9574 - mae: 4.2316 - val_loss: 32.5684 - val_mae: 3.6576 - lr: 1.0000e-04\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 79.3045 - mae: 4.1590 - val_loss: 32.2521 - val_mae: 3.6149 - lr: 1.0000e-04\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 76.6920 - mae: 4.0788 - val_loss: 27.8050 - val_mae: 3.3230 - lr: 1.0000e-04\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 73.8135 - mae: 3.9433 - val_loss: 29.2575 - val_mae: 3.4594 - lr: 1.0000e-04\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 72.4449 - mae: 3.9370 - val_loss: 26.5911 - val_mae: 3.2333 - lr: 1.0000e-04\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 71.3137 - mae: 3.9610 - val_loss: 28.0707 - val_mae: 3.3195 - lr: 1.0000e-04\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 68.9556 - mae: 3.8209 - val_loss: 25.5085 - val_mae: 3.2746 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 66.9498 - mae: 3.7159 - val_loss: 24.4537 - val_mae: 3.1019 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 237s 6s/step - loss: 65.5799 - mae: 3.6747 - val_loss: 23.8031 - val_mae: 3.0619 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 64.5477 - mae: 3.6618 - val_loss: 24.2724 - val_mae: 3.2108 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 63.0729 - mae: 3.5860 - val_loss: 23.2206 - val_mae: 3.0246 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 61.9211 - mae: 3.5362 - val_loss: 22.2928 - val_mae: 2.9597 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 60.7846 - mae: 3.4941 - val_loss: 22.5643 - val_mae: 3.0233 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 60.2849 - mae: 3.5079 - val_loss: 22.8519 - val_mae: 3.0242 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 59.0120 - mae: 3.4457 - val_loss: 21.6032 - val_mae: 2.9567 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 58.1674 - mae: 3.4269 - val_loss: 21.1064 - val_mae: 2.8783 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 57.0211 - mae: 3.3610 - val_loss: 21.0053 - val_mae: 2.8579 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 56.3171 - mae: 3.3498 - val_loss: 21.4895 - val_mae: 2.8931 - lr: 1.0000e-04\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 55.8905 - mae: 3.3642 - val_loss: 21.0977 - val_mae: 2.9344 - lr: 1.0000e-04\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 55.3285 - mae: 3.3606 - val_loss: 20.3375 - val_mae: 2.8852 - lr: 1.0000e-04\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 54.4693 - mae: 3.3033 - val_loss: 20.1073 - val_mae: 2.7868 - lr: 1.0000e-04\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 53.4239 - mae: 3.2456 - val_loss: 20.4838 - val_mae: 2.8435 - lr: 1.0000e-04\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 52.9417 - mae: 3.2499 - val_loss: 19.9899 - val_mae: 2.8709 - lr: 1.0000e-04\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 52.4009 - mae: 3.2385 - val_loss: 19.0451 - val_mae: 2.7029 - lr: 1.0000e-04\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 51.8389 - mae: 3.2079 - val_loss: 19.5910 - val_mae: 2.7720 - lr: 1.0000e-04\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 51.3105 - mae: 3.1999 - val_loss: 18.9181 - val_mae: 2.6997 - lr: 1.0000e-04\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 239s 6s/step - loss: 50.4183 - mae: 3.1356 - val_loss: 18.7188 - val_mae: 2.6820 - lr: 1.0000e-04\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 50.1567 - mae: 3.1601 - val_loss: 18.5667 - val_mae: 2.6645 - lr: 1.0000e-04\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 49.4740 - mae: 3.1134 - val_loss: 18.3581 - val_mae: 2.6382 - lr: 1.0000e-04\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 48.9624 - mae: 3.0906 - val_loss: 17.9335 - val_mae: 2.6311 - lr: 1.0000e-04\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 48.5606 - mae: 3.0842 - val_loss: 18.2045 - val_mae: 2.6322 - lr: 1.0000e-04\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 48.2622 - mae: 3.0930 - val_loss: 17.8664 - val_mae: 2.6126 - lr: 1.0000e-04\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 47.8872 - mae: 3.0769 - val_loss: 17.7025 - val_mae: 2.5960 - lr: 1.0000e-04\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 47.1229 - mae: 3.0388 - val_loss: 18.0030 - val_mae: 2.6909 - lr: 1.0000e-04\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 46.7010 - mae: 3.0303 - val_loss: 18.1534 - val_mae: 2.6724 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 46.4156 - mae: 3.0168 - val_loss: 17.2042 - val_mae: 2.5477 - lr: 1.0000e-04\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 46.2437 - mae: 3.0277 - val_loss: 17.0878 - val_mae: 2.5413 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 45.6235 - mae: 2.9855 - val_loss: 17.8551 - val_mae: 2.5929 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 45.4566 - mae: 3.0034 - val_loss: 17.4445 - val_mae: 2.6341 - lr: 1.0000e-04\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 242s 6s/step - loss: 44.7279 - mae: 2.9492 - val_loss: 17.1935 - val_mae: 2.7023 - lr: 1.0000e-04\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 44.7337 - mae: 2.9868 - val_loss: 16.3399 - val_mae: 2.5008 - lr: 1.0000e-04\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 43.9285 - mae: 2.9179 - val_loss: 16.2097 - val_mae: 2.4988 - lr: 1.0000e-04\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 43.5217 - mae: 2.8918 - val_loss: 15.9401 - val_mae: 2.4509 - lr: 1.0000e-04\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 43.5226 - mae: 2.9235 - val_loss: 17.4077 - val_mae: 2.6012 - lr: 1.0000e-04\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 43.0166 - mae: 2.8991 - val_loss: 15.6238 - val_mae: 2.4352 - lr: 1.0000e-04\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 42.5541 - mae: 2.8507 - val_loss: 15.6495 - val_mae: 2.4214 - lr: 1.0000e-04\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 42.1163 - mae: 2.8404 - val_loss: 15.4787 - val_mae: 2.4592 - lr: 1.0000e-04\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 41.9073 - mae: 2.8398 - val_loss: 15.4174 - val_mae: 2.4017 - lr: 1.0000e-04\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 41.6196 - mae: 2.8324 - val_loss: 16.3897 - val_mae: 2.4800 - lr: 1.0000e-04\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 41.2069 - mae: 2.8146 - val_loss: 15.0422 - val_mae: 2.4115 - lr: 1.0000e-04\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 41.1154 - mae: 2.8044 - val_loss: 15.6780 - val_mae: 2.4275 - lr: 1.0000e-04\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 40.8735 - mae: 2.8077 - val_loss: 15.4893 - val_mae: 2.4486 - lr: 1.0000e-04\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 40.4675 - mae: 2.7788 - val_loss: 14.6370 - val_mae: 2.3547 - lr: 1.0000e-04\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 40.0707 - mae: 2.7616 - val_loss: 14.7241 - val_mae: 2.3465 - lr: 1.0000e-04\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 40.0695 - mae: 2.7953 - val_loss: 14.5496 - val_mae: 2.3765 - lr: 1.0000e-04\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 39.5032 - mae: 2.7350 - val_loss: 14.7209 - val_mae: 2.3365 - lr: 1.0000e-04\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 39.5052 - mae: 2.7490 - val_loss: 14.9434 - val_mae: 2.4883 - lr: 1.0000e-04\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 39.0936 - mae: 2.7301 - val_loss: 14.0802 - val_mae: 2.2932 - lr: 1.0000e-04\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 38.8724 - mae: 2.7239 - val_loss: 14.9889 - val_mae: 2.4213 - lr: 1.0000e-04\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 38.5265 - mae: 2.7154 - val_loss: 13.7756 - val_mae: 2.2681 - lr: 1.0000e-04\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 38.4698 - mae: 2.7149 - val_loss: 14.4002 - val_mae: 2.3569 - lr: 1.0000e-04\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 38.5642 - mae: 2.7691 - val_loss: 14.0718 - val_mae: 2.3108 - lr: 1.0000e-04\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 281s 7s/step - loss: 38.1002 - mae: 2.6998 - val_loss: 13.8304 - val_mae: 2.2794 - lr: 1.0000e-04\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 37.7887 - mae: 2.7018 - val_loss: 14.9049 - val_mae: 2.5777 - lr: 1.0000e-04\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 37.7683 - mae: 2.7206 - val_loss: 13.8710 - val_mae: 2.3696 - lr: 1.0000e-04\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 37.5714 - mae: 2.7117 - val_loss: 13.3166 - val_mae: 2.2368 - lr: 1.0000e-04\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 37.0110 - mae: 2.6592 - val_loss: 13.5789 - val_mae: 2.2546 - lr: 1.0000e-04\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 37.0038 - mae: 2.6931 - val_loss: 13.8818 - val_mae: 2.2614 - lr: 1.0000e-04\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 37.0184 - mae: 2.6798 - val_loss: 13.2839 - val_mae: 2.2256 - lr: 1.0000e-04\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 36.3328 - mae: 2.6321 - val_loss: 13.0153 - val_mae: 2.2112 - lr: 1.0000e-04\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 36.2672 - mae: 2.6410 - val_loss: 13.0296 - val_mae: 2.2294 - lr: 1.0000e-04\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 35.9695 - mae: 2.6267 - val_loss: 12.9240 - val_mae: 2.1903 - lr: 1.0000e-04\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 36.0856 - mae: 2.6537 - val_loss: 12.8952 - val_mae: 2.1959 - lr: 1.0000e-04\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 35.6311 - mae: 2.6376 - val_loss: 12.6708 - val_mae: 2.1803 - lr: 1.0000e-04\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 35.2700 - mae: 2.6047 - val_loss: 12.7764 - val_mae: 2.2018 - lr: 1.0000e-04\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 35.5132 - mae: 2.6477 - val_loss: 15.0094 - val_mae: 2.5823 - lr: 1.0000e-04\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 236s 6s/step - loss: 35.4101 - mae: 2.6648 - val_loss: 12.7314 - val_mae: 2.1717 - lr: 1.0000e-04\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 34.8185 - mae: 2.6053 - val_loss: 12.7169 - val_mae: 2.1749 - lr: 1.0000e-04\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 34.3976 - mae: 2.5709 - val_loss: 13.3742 - val_mae: 2.2783 - lr: 1.0000e-04\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 34.3191 - mae: 2.5820 - val_loss: 12.5533 - val_mae: 2.1556 - lr: 1.0000e-04\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 33.9299 - mae: 2.5586 - val_loss: 12.6152 - val_mae: 2.1651 - lr: 1.0000e-04\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 33.9747 - mae: 2.5764 - val_loss: 12.9054 - val_mae: 2.2548 - lr: 1.0000e-04\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 33.5580 - mae: 2.5496 - val_loss: 12.4990 - val_mae: 2.1577 - lr: 1.0000e-04\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 33.4490 - mae: 2.5544 - val_loss: 12.8872 - val_mae: 2.2248 - lr: 1.0000e-04\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 33.4164 - mae: 2.5816 - val_loss: 12.5400 - val_mae: 2.1606 - lr: 1.0000e-04\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 33.4164 - mae: 2.6046 - val_loss: 13.7649 - val_mae: 2.3177 - lr: 1.0000e-04\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 227s 5s/step - loss: 33.0735 - mae: 2.5613 - val_loss: 12.2880 - val_mae: 2.1373 - lr: 1.0000e-04\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 212s 5s/step - loss: 32.4271 - mae: 2.5145 - val_loss: 12.3754 - val_mae: 2.1511 - lr: 1.0000e-04\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 158s 4s/step - loss: 32.3807 - mae: 2.5158 - val_loss: 12.0240 - val_mae: 2.1004 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# from sst and rainfall ---> rainfall\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Conv2D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Lambda\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2DTranspose, TimeDistributed\n",
    "from keras import callbacks\n",
    "import xarray as xr\n",
    "\n",
    "# Define global variables\n",
    "lead_time = 1\n",
    "save_path = \"/home/cccr/roxy/matin/MTech_project/model/Conv-LSTM/7in2out/\"\n",
    "model_path = save_path + \"ConvLstm_sst0rf_rf_1SI.h5\"\n",
    "add_data = \"/home/cccr/roxy/matin/MTech_project/data/\"\n",
    "\n",
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"1998-01-01\",\"2016-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_0lag.nc\",\"SIFilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n",
    "# Perform additional processing or modeling steps as needed\n",
    "seq = tf.keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "seq.add(ConvLSTM2D(filters=4, kernel_size=(3,3), padding='same', input_shape=(7,25,39,2), return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=False, data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=15, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "# Create the query, value, and key inputs\n",
    "query_input = Input(shape=(25,39,2), name='query_input')\n",
    "value_input = Input(shape=(25,39,2), name='value_input')\n",
    "key_input = Input(shape=(25,39,2), name='key_input')\n",
    "\n",
    "# Pass the inputs to the Attention layer\n",
    "attention_output = Attention(name='attention')([query_input, value_input, key_input])\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=10**-4)\n",
    "seq.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae'])\n",
    "\n",
    "print(seq.summary())\n",
    "\n",
    "# Define the number of training epochs\n",
    "n_epochs = 200\n",
    "logdir = save_path\n",
    "# Define the TensorBoard callback for monitoring training progress\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=10, batch_size=10, write_graph=True)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback for reducing the learning rate when the model plateaus\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=10**-20)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation mean absolute error\n",
    "checkpoint_callback_mae = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_mae',\n",
    "                                                             mode='min', save_best_only=True)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation loss\n",
    "checkpoint_callback_loss = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss',\n",
    "                                                              mode='min', save_best_only=True)\n",
    "\n",
    "# Define the EarlyStopping callback for stopping training when the model stops improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "# Define the TerminateOnNaN callback for terminating the training process if there is NaN value in any of the output\n",
    "terminate_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "# Train the model using the defined callbacks\n",
    "history = seq.fit(x, y, epochs=n_epochs, validation_split=0.1,\n",
    "                  batch_size=50, callbacks=[early_stop_callback,\n",
    "                  checkpoint_callback_mae, checkpoint_callback_loss,\n",
    "                  tb_callback, lr_callback, terminate_callback])\n",
    "#Save the trained model\n",
    "seq.save(model_path)\n",
    "\n",
    "#Save the training history\n",
    "\n",
    "np.save(f'{save_path}ConvLstm_sst0rf_rf_1SI.npy', history.history)               \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb53a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (236, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (236, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (236, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (236, 1, 25, 39)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"2017-01-01\",\"2018-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_0lag.nc\",\"SIFilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2a2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 5s 374ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.84253291],\n",
       "       [0.84253291, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "pred = model.predict(x)\n",
    "\n",
    "actual = y.flatten()\n",
    "prediction = pred.flatten()\n",
    "corr = np.corrcoef(actual,prediction)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
