{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce65e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (2308, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (2308, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (2308, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (2308, 1, 25, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 16:32:43.760586: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 7, 25, 39, 4)      880       \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 7, 25, 39, 8)      3488      \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 7, 25, 39, 8)      4640      \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 7, 25, 39, 16)     13888     \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 25, 39, 16)        18496     \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 25, 39, 15)        2175      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 39, 1)         136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,703\n",
      "Trainable params: 43,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/200\n",
      "42/42 [==============================] - 278s 6s/step - loss: 2579.3774 - mae: 39.6650 - val_loss: 2408.4724 - val_mae: 39.0074 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 2508.5259 - mae: 38.7628 - val_loss: 2196.2754 - val_mae: 36.1666 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 239s 6s/step - loss: 2178.0320 - mae: 34.2280 - val_loss: 1856.0890 - val_mae: 31.1674 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1874.8866 - mae: 29.6691 - val_loss: 1584.0538 - val_mae: 26.7517 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 1609.1525 - mae: 25.5825 - val_loss: 1342.8951 - val_mae: 22.7200 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1385.4780 - mae: 22.4612 - val_loss: 1156.7899 - val_mae: 19.9017 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 1223.7374 - mae: 20.6670 - val_loss: 1030.0991 - val_mae: 18.4723 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 1115.8741 - mae: 19.9921 - val_loss: 958.5626 - val_mae: 18.2479 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 236s 6s/step - loss: 1062.5859 - mae: 20.2208 - val_loss: 933.2013 - val_mae: 18.7185 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1046.8049 - mae: 20.6662 - val_loss: 930.1444 - val_mae: 19.1019 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1044.4600 - mae: 20.9198 - val_loss: 930.2236 - val_mae: 19.2301 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1044.1525 - mae: 20.9888 - val_loss: 930.2101 - val_mae: 19.2595 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1043.9626 - mae: 21.0048 - val_loss: 930.1089 - val_mae: 19.2660 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1043.9066 - mae: 20.9808 - val_loss: 929.9442 - val_mae: 19.2570 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1043.7249 - mae: 21.0375 - val_loss: 930.0420 - val_mae: 19.3025 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1043.6553 - mae: 20.9537 - val_loss: 929.6260 - val_mae: 19.2418 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 1043.3658 - mae: 20.9979 - val_loss: 929.7399 - val_mae: 19.2929 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 1043.2767 - mae: 21.0178 - val_loss: 929.5306 - val_mae: 19.2766 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1043.1393 - mae: 21.0029 - val_loss: 929.2251 - val_mae: 19.2381 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 1043.0452 - mae: 20.9526 - val_loss: 929.1173 - val_mae: 19.2431 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 1043.0319 - mae: 21.0294 - val_loss: 929.1335 - val_mae: 19.2742 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1042.8240 - mae: 20.9692 - val_loss: 928.7919 - val_mae: 19.2282 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 1042.6288 - mae: 20.9570 - val_loss: 928.7402 - val_mae: 19.2478 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 1042.6354 - mae: 21.0578 - val_loss: 928.7721 - val_mae: 19.2816 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1042.3993 - mae: 20.9488 - val_loss: 928.3436 - val_mae: 19.2197 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 1042.1499 - mae: 20.9598 - val_loss: 928.2354 - val_mae: 19.2263 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1042.0858 - mae: 20.9968 - val_loss: 928.2515 - val_mae: 19.2606 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1041.9175 - mae: 20.9987 - val_loss: 928.1795 - val_mae: 19.2733 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 233s 6s/step - loss: 1041.9443 - mae: 20.9486 - val_loss: 927.7789 - val_mae: 19.2193 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1041.6555 - mae: 20.9445 - val_loss: 927.6625 - val_mae: 19.2259 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 1041.5964 - mae: 21.0130 - val_loss: 927.8494 - val_mae: 19.2887 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1041.3212 - mae: 20.9780 - val_loss: 927.2656 - val_mae: 19.2006 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1041.2676 - mae: 20.9891 - val_loss: 927.0953 - val_mae: 19.1950 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1041.2169 - mae: 20.9766 - val_loss: 927.0168 - val_mae: 19.2124 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 1041.0464 - mae: 20.9107 - val_loss: 926.7957 - val_mae: 19.1971 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 1040.8151 - mae: 20.9846 - val_loss: 926.6592 - val_mae: 19.2011 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 1040.8646 - mae: 21.0093 - val_loss: 926.8192 - val_mae: 19.2625 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 1040.5266 - mae: 20.9173 - val_loss: 926.4006 - val_mae: 19.2105 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 1040.3228 - mae: 20.9698 - val_loss: 926.1377 - val_mae: 19.1849 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1040.1580 - mae: 20.9606 - val_loss: 925.9073 - val_mae: 19.1649 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1039.9114 - mae: 20.8978 - val_loss: 925.8011 - val_mae: 19.1800 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1039.9044 - mae: 20.8652 - val_loss: 925.7300 - val_mae: 19.2000 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1039.8187 - mae: 21.0529 - val_loss: 925.7134 - val_mae: 19.2290 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 1039.3685 - mae: 20.9238 - val_loss: 925.1844 - val_mae: 19.1437 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1039.3553 - mae: 20.9374 - val_loss: 925.0904 - val_mae: 19.1622 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1039.1722 - mae: 20.9014 - val_loss: 924.9031 - val_mae: 19.1556 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 1039.1884 - mae: 20.9563 - val_loss: 925.0443 - val_mae: 19.2218 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1038.8762 - mae: 20.9233 - val_loss: 924.4536 - val_mae: 19.1215 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 1038.9449 - mae: 20.9390 - val_loss: 924.3643 - val_mae: 19.1435 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1038.4252 - mae: 20.9298 - val_loss: 924.3960 - val_mae: 19.1902 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1038.3197 - mae: 20.8899 - val_loss: 924.1359 - val_mae: 19.1687 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 236s 6s/step - loss: 1038.1246 - mae: 20.9362 - val_loss: 924.1044 - val_mae: 19.1968 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 1037.9902 - mae: 20.8858 - val_loss: 923.7731 - val_mae: 19.1621 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1037.9681 - mae: 20.9716 - val_loss: 923.6219 - val_mae: 19.1645 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1037.8739 - mae: 20.8474 - val_loss: 923.5457 - val_mae: 19.1835 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 1037.5940 - mae: 20.9104 - val_loss: 923.4219 - val_mae: 19.1911 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 1037.2578 - mae: 20.9010 - val_loss: 922.9973 - val_mae: 19.1354 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1037.1373 - mae: 20.9146 - val_loss: 922.9866 - val_mae: 19.1702 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1037.0490 - mae: 20.8669 - val_loss: 922.6765 - val_mae: 19.1390 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1036.8204 - mae: 20.9423 - val_loss: 922.6786 - val_mae: 19.1752 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 1036.7676 - mae: 20.9581 - val_loss: 922.4688 - val_mae: 19.1675 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1036.5643 - mae: 20.8568 - val_loss: 922.0942 - val_mae: 19.1225 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 1036.3372 - mae: 20.9191 - val_loss: 922.0367 - val_mae: 19.1485 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 1036.4509 - mae: 20.8975 - val_loss: 922.0148 - val_mae: 19.1761 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 1036.0632 - mae: 20.8171 - val_loss: 921.6270 - val_mae: 19.1325 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 238s 6s/step - loss: 1035.7885 - mae: 20.8730 - val_loss: 921.4464 - val_mae: 19.1314 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1035.6730 - mae: 20.9501 - val_loss: 921.4700 - val_mae: 19.1695 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 1035.6317 - mae: 20.8602 - val_loss: 920.9868 - val_mae: 19.1047 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1035.2972 - mae: 20.8804 - val_loss: 920.8037 - val_mae: 19.1021 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 1035.1078 - mae: 20.8222 - val_loss: 920.7275 - val_mae: 19.1241 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 1035.1918 - mae: 20.8686 - val_loss: 920.4084 - val_mae: 19.0898 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 1034.7952 - mae: 20.8974 - val_loss: 920.4476 - val_mae: 19.1369 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1034.8115 - mae: 20.8492 - val_loss: 920.2936 - val_mae: 19.1396 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1034.4709 - mae: 20.9096 - val_loss: 919.8171 - val_mae: 19.0705 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1034.3661 - mae: 20.8122 - val_loss: 919.9404 - val_mae: 19.1372 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1034.2581 - mae: 20.9451 - val_loss: 919.4598 - val_mae: 19.0691 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 1034.3533 - mae: 20.7305 - val_loss: 919.6401 - val_mae: 19.1445 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 1034.0157 - mae: 20.8654 - val_loss: 919.8489 - val_mae: 19.2043 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1033.5870 - mae: 20.8652 - val_loss: 918.9894 - val_mae: 19.0808 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1033.5167 - mae: 20.8240 - val_loss: 918.9501 - val_mae: 19.1099 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 1033.3916 - mae: 20.8472 - val_loss: 918.7673 - val_mae: 19.1069 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 1033.2362 - mae: 20.9364 - val_loss: 918.6917 - val_mae: 19.1247 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 1032.8804 - mae: 20.8208 - val_loss: 918.1608 - val_mae: 19.0442 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1032.9219 - mae: 20.8460 - val_loss: 918.1560 - val_mae: 19.0848 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 1032.5781 - mae: 20.8338 - val_loss: 918.0430 - val_mae: 19.0974 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1032.7441 - mae: 20.7395 - val_loss: 917.8969 - val_mae: 19.1013 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1032.4470 - mae: 20.9505 - val_loss: 917.7003 - val_mae: 19.0949 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 1032.0282 - mae: 20.8002 - val_loss: 917.1495 - val_mae: 18.9979 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 1032.0439 - mae: 20.7311 - val_loss: 917.2931 - val_mae: 19.0807 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1032.0343 - mae: 20.9961 - val_loss: 917.2209 - val_mae: 19.0986 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1031.8252 - mae: 20.7776 - val_loss: 916.6824 - val_mae: 19.0123 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 1031.5226 - mae: 20.7819 - val_loss: 916.5536 - val_mae: 19.0230 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1031.3320 - mae: 20.8285 - val_loss: 916.7610 - val_mae: 19.1033 - lr: 1.0000e-04\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 1031.1478 - mae: 20.8511 - val_loss: 916.1106 - val_mae: 18.9906 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1030.9194 - mae: 20.7697 - val_loss: 916.1780 - val_mae: 19.0533 - lr: 1.0000e-04\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 243s 6s/step - loss: 1030.8250 - mae: 20.7956 - val_loss: 916.3156 - val_mae: 19.1104 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 1030.8088 - mae: 20.8908 - val_loss: 915.4825 - val_mae: 18.9439 - lr: 1.0000e-04\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1030.4806 - mae: 20.7691 - val_loss: 915.6970 - val_mae: 19.0554 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 234s 6s/step - loss: 1030.3843 - mae: 20.8533 - val_loss: 915.6300 - val_mae: 19.0748 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1030.2291 - mae: 20.8067 - val_loss: 915.3845 - val_mae: 19.0580 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1030.2865 - mae: 20.7431 - val_loss: 915.4863 - val_mae: 19.1049 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 1029.8566 - mae: 20.8035 - val_loss: 915.1680 - val_mae: 19.0770 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1029.7037 - mae: 20.8897 - val_loss: 914.5841 - val_mae: 18.9811 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 1029.6176 - mae: 20.7296 - val_loss: 914.6692 - val_mae: 19.0418 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1029.3218 - mae: 20.7884 - val_loss: 914.4835 - val_mae: 19.0368 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 1029.3505 - mae: 20.8194 - val_loss: 913.9980 - val_mae: 18.9510 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1029.2434 - mae: 20.8437 - val_loss: 913.9929 - val_mae: 18.9978 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1028.9679 - mae: 20.6710 - val_loss: 913.8651 - val_mae: 19.0059 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 1028.8341 - mae: 20.8345 - val_loss: 913.6388 - val_mae: 18.9887 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1028.6204 - mae: 20.7013 - val_loss: 913.5614 - val_mae: 19.0087 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1028.8844 - mae: 20.8344 - val_loss: 914.2761 - val_mae: 19.1538 - lr: 1.0000e-04\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 1028.2692 - mae: 20.9120 - val_loss: 913.1498 - val_mae: 18.9860 - lr: 1.0000e-04\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 1028.2539 - mae: 20.6290 - val_loss: 912.9315 - val_mae: 18.9699 - lr: 1.0000e-04\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1028.0918 - mae: 20.8044 - val_loss: 913.6561 - val_mae: 19.1303 - lr: 1.0000e-04\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1028.0372 - mae: 20.7778 - val_loss: 912.3882 - val_mae: 18.8839 - lr: 1.0000e-04\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 239s 6s/step - loss: 1027.6644 - mae: 20.7787 - val_loss: 912.6557 - val_mae: 19.0150 - lr: 1.0000e-04\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1027.5380 - mae: 20.7479 - val_loss: 912.3056 - val_mae: 18.9706 - lr: 1.0000e-04\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1027.7194 - mae: 20.7911 - val_loss: 912.2471 - val_mae: 18.9938 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 1027.4408 - mae: 20.6684 - val_loss: 912.2895 - val_mae: 19.0317 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1027.0433 - mae: 20.8178 - val_loss: 912.1961 - val_mae: 19.0412 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 1026.8727 - mae: 20.7625 - val_loss: 911.8019 - val_mae: 18.9932 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 1026.8607 - mae: 20.7115 - val_loss: 911.8645 - val_mae: 19.0351 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1026.6661 - mae: 20.7497 - val_loss: 911.9246 - val_mae: 19.0690 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1026.6655 - mae: 20.7450 - val_loss: 911.2485 - val_mae: 18.9678 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1026.3871 - mae: 20.8038 - val_loss: 911.2677 - val_mae: 19.0048 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1026.4591 - mae: 20.7041 - val_loss: 911.3625 - val_mae: 19.0471 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 1025.9597 - mae: 20.7684 - val_loss: 910.5859 - val_mae: 18.9053 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1026.0778 - mae: 20.6368 - val_loss: 910.6843 - val_mae: 18.9734 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1025.7908 - mae: 20.8230 - val_loss: 910.5208 - val_mae: 18.9680 - lr: 1.0000e-04\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 1025.5392 - mae: 20.7292 - val_loss: 910.4398 - val_mae: 18.9799 - lr: 1.0000e-04\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 1025.7216 - mae: 20.7587 - val_loss: 910.3456 - val_mae: 18.9899 - lr: 1.0000e-04\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 1025.3328 - mae: 20.6733 - val_loss: 910.0811 - val_mae: 18.9634 - lr: 1.0000e-04\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1025.2063 - mae: 20.7377 - val_loss: 910.1221 - val_mae: 19.0006 - lr: 1.0000e-04\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 1025.3633 - mae: 20.7331 - val_loss: 909.7263 - val_mae: 18.9464 - lr: 1.0000e-04\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1025.0386 - mae: 20.7834 - val_loss: 909.7819 - val_mae: 18.9874 - lr: 1.0000e-04\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1024.7393 - mae: 20.7257 - val_loss: 909.3923 - val_mae: 18.9312 - lr: 1.0000e-04\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1024.5878 - mae: 20.7228 - val_loss: 909.5289 - val_mae: 18.9909 - lr: 1.0000e-04\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 1024.6749 - mae: 20.7816 - val_loss: 909.2768 - val_mae: 18.9677 - lr: 1.0000e-04\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 1024.3286 - mae: 20.6776 - val_loss: 908.7857 - val_mae: 18.8729 - lr: 1.0000e-04\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 235s 6s/step - loss: 1024.1360 - mae: 20.6750 - val_loss: 909.2199 - val_mae: 19.0063 - lr: 1.0000e-04\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 1024.2738 - mae: 20.7301 - val_loss: 909.3901 - val_mae: 19.0543 - lr: 1.0000e-04\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 233s 6s/step - loss: 1024.0629 - mae: 20.7787 - val_loss: 908.7451 - val_mae: 18.9661 - lr: 1.0000e-04\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 1023.9056 - mae: 20.6596 - val_loss: 908.4642 - val_mae: 18.9334 - lr: 1.0000e-04\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1023.8279 - mae: 20.7724 - val_loss: 908.5302 - val_mae: 18.9742 - lr: 1.0000e-04\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1023.8250 - mae: 20.7580 - val_loss: 907.9944 - val_mae: 18.8738 - lr: 1.0000e-04\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1023.6359 - mae: 20.5744 - val_loss: 908.4421 - val_mae: 19.0028 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 1023.4503 - mae: 20.8172 - val_loss: 908.0759 - val_mae: 18.9583 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1023.4301 - mae: 20.6899 - val_loss: 907.8315 - val_mae: 18.9335 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1023.1013 - mae: 20.7355 - val_loss: 907.8367 - val_mae: 18.9597 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 238s 6s/step - loss: 1023.2080 - mae: 20.7134 - val_loss: 907.5694 - val_mae: 18.9292 - lr: 1.0000e-04\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 239s 6s/step - loss: 1023.1422 - mae: 20.6797 - val_loss: 907.8296 - val_mae: 19.0022 - lr: 1.0000e-04\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 1022.7321 - mae: 20.7663 - val_loss: 907.2003 - val_mae: 18.8997 - lr: 1.0000e-04\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 1022.6440 - mae: 20.6318 - val_loss: 907.2061 - val_mae: 18.9288 - lr: 1.0000e-04\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 1022.5090 - mae: 20.7204 - val_loss: 907.1550 - val_mae: 18.9434 - lr: 1.0000e-04\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 1022.4066 - mae: 20.6761 - val_loss: 907.0981 - val_mae: 18.9558 - lr: 1.0000e-04\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 1022.3043 - mae: 20.7132 - val_loss: 906.9355 - val_mae: 18.9476 - lr: 1.0000e-04\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 1022.4478 - mae: 20.6953 - val_loss: 906.5827 - val_mae: 18.8955 - lr: 1.0000e-04\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 1022.0735 - mae: 20.6763 - val_loss: 906.4200 - val_mae: 18.8827 - lr: 1.0000e-04\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 1022.0685 - mae: 20.7410 - val_loss: 906.2521 - val_mae: 18.8691 - lr: 1.0000e-04\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 1021.9200 - mae: 20.6121 - val_loss: 906.2998 - val_mae: 18.9107 - lr: 1.0000e-04\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 1021.7920 - mae: 20.6363 - val_loss: 906.2814 - val_mae: 18.9322 - lr: 1.0000e-04\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1021.7120 - mae: 20.7011 - val_loss: 906.0958 - val_mae: 18.9153 - lr: 1.0000e-04\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 1021.8564 - mae: 20.8171 - val_loss: 906.1957 - val_mae: 18.9582 - lr: 1.0000e-04\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 1021.5661 - mae: 20.6144 - val_loss: 905.7465 - val_mae: 18.8879 - lr: 1.0000e-04\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 1021.2434 - mae: 20.6279 - val_loss: 905.9174 - val_mae: 18.9470 - lr: 1.0000e-04\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 1021.1508 - mae: 20.6987 - val_loss: 906.0254 - val_mae: 18.9852 - lr: 1.0000e-04\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1021.1273 - mae: 20.7607 - val_loss: 905.9255 - val_mae: 18.9872 - lr: 1.0000e-04\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1021.0861 - mae: 20.6559 - val_loss: 905.1623 - val_mae: 18.8662 - lr: 1.0000e-04\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 1014.0082 - mae: 20.3479 - val_loss: 876.6760 - val_mae: 18.1135 - lr: 1.0000e-04\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 975.2358 - mae: 19.5894 - val_loss: 830.6699 - val_mae: 16.6761 - lr: 1.0000e-04\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 922.6866 - mae: 18.6587 - val_loss: 782.0642 - val_mae: 16.7703 - lr: 1.0000e-04\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 860.4344 - mae: 17.4836 - val_loss: 718.3842 - val_mae: 14.3399 - lr: 1.0000e-04\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 794.1003 - mae: 16.4955 - val_loss: 662.6472 - val_mae: 14.2668 - lr: 1.0000e-04\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 740.8618 - mae: 15.7036 - val_loss: 617.4896 - val_mae: 13.7804 - lr: 1.0000e-04\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 695.2227 - mae: 15.0768 - val_loss: 584.8187 - val_mae: 13.4513 - lr: 1.0000e-04\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 657.7260 - mae: 14.5704 - val_loss: 599.9200 - val_mae: 14.6280 - lr: 1.0000e-04\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 629.3045 - mae: 14.2725 - val_loss: 542.0035 - val_mae: 12.8923 - lr: 1.0000e-04\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 618.5783 - mae: 14.1730 - val_loss: 516.8896 - val_mae: 12.2498 - lr: 1.0000e-04\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 592.7641 - mae: 13.8473 - val_loss: 503.4782 - val_mae: 12.7249 - lr: 1.0000e-04\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 566.0136 - mae: 13.4780 - val_loss: 478.2618 - val_mae: 11.9594 - lr: 1.0000e-04\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 552.7944 - mae: 13.3076 - val_loss: 481.8164 - val_mae: 12.5734 - lr: 1.0000e-04\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 538.9432 - mae: 13.1506 - val_loss: 444.6193 - val_mae: 11.5130 - lr: 1.0000e-04\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 530.3109 - mae: 13.0279 - val_loss: 436.4446 - val_mae: 11.2298 - lr: 1.0000e-04\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 517.1260 - mae: 12.8035 - val_loss: 452.1111 - val_mae: 11.4857 - lr: 1.0000e-04\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 519.4802 - mae: 12.9001 - val_loss: 424.3189 - val_mae: 11.5915 - lr: 1.0000e-04\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 500.1525 - mae: 12.5881 - val_loss: 397.9309 - val_mae: 10.8198 - lr: 1.0000e-04\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 487.9043 - mae: 12.3880 - val_loss: 381.6218 - val_mae: 10.6088 - lr: 1.0000e-04\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 238s 6s/step - loss: 475.7917 - mae: 12.1692 - val_loss: 382.3238 - val_mae: 10.4624 - lr: 1.0000e-04\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 466.3136 - mae: 12.0143 - val_loss: 365.0432 - val_mae: 10.2025 - lr: 1.0000e-04\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 462.6479 - mae: 11.9897 - val_loss: 359.7902 - val_mae: 10.0389 - lr: 1.0000e-04\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 450.6717 - mae: 11.7606 - val_loss: 349.4591 - val_mae: 9.9016 - lr: 1.0000e-04\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 443.4118 - mae: 11.6696 - val_loss: 331.4904 - val_mae: 9.6894 - lr: 1.0000e-04\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 436.3051 - mae: 11.5597 - val_loss: 347.3216 - val_mae: 9.8514 - lr: 1.0000e-04\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 427.9185 - mae: 11.4177 - val_loss: 330.8281 - val_mae: 9.5204 - lr: 1.0000e-04\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 422.5802 - mae: 11.3246 - val_loss: 313.6540 - val_mae: 9.3558 - lr: 1.0000e-04\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 415.0303 - mae: 11.2421 - val_loss: 313.5407 - val_mae: 9.2975 - lr: 1.0000e-04\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 406.1166 - mae: 11.0675 - val_loss: 298.5197 - val_mae: 8.9922 - lr: 1.0000e-04\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 399.7678 - mae: 10.9591 - val_loss: 288.6199 - val_mae: 9.0287 - lr: 1.0000e-04\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 395.0109 - mae: 10.8884 - val_loss: 284.0091 - val_mae: 8.8901 - lr: 1.0000e-04\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 248s 6s/step - loss: 391.1817 - mae: 10.8662 - val_loss: 282.5121 - val_mae: 8.7564 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# from sst and rainfall ---> rainfall\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Conv2D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Lambda\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2DTranspose, TimeDistributed\n",
    "from keras import callbacks\n",
    "import xarray as xr\n",
    "\n",
    "# Define global variables\n",
    "lead_time = 3\n",
    "save_path = \"/home/cccr/roxy/matin/MTech_project/model/Conv-LSTM/7in4out/\"\n",
    "model_path = save_path + \"ConvLstm_sst0rf_rf_3bob.h5\"\n",
    "add_data = \"/home/cccr/roxy/matin/MTech_project/data/\"\n",
    "\n",
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"1998-01-01\",\"2016-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "channels = [\"FilteredrfBOB_0lag.nc\",\"FilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"FilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n",
    "# Perform additional processing or modeling steps as needed\n",
    "seq = tf.keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "seq.add(ConvLSTM2D(filters=4, kernel_size=(3,3), padding='same', input_shape=(7,25,39,2), return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=False, data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=15, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "# Create the query, value, and key inputs\n",
    "query_input = Input(shape=(25,39,2), name='query_input')\n",
    "value_input = Input(shape=(25,39,2), name='value_input')\n",
    "key_input = Input(shape=(25,39,2), name='key_input')\n",
    "\n",
    "# Pass the inputs to the Attention layer\n",
    "attention_output = Attention(name='attention')([query_input, value_input, key_input])\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=10**-4)\n",
    "seq.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae'])\n",
    "\n",
    "print(seq.summary())\n",
    "\n",
    "# Define the number of training epochs\n",
    "n_epochs = 200\n",
    "logdir = save_path\n",
    "# Define the TensorBoard callback for monitoring training progress\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=10, batch_size=10, write_graph=True)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback for reducing the learning rate when the model plateaus\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=10**-20)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation mean absolute error\n",
    "checkpoint_callback_mae = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_mae',\n",
    "                                                             mode='min', save_best_only=True)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation loss\n",
    "checkpoint_callback_loss = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss',\n",
    "                                                              mode='min', save_best_only=True)\n",
    "\n",
    "# Define the EarlyStopping callback for stopping training when the model stops improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "# Define the TerminateOnNaN callback for terminating the training process if there is NaN value in any of the output\n",
    "terminate_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "# Train the model using the defined callbacks\n",
    "history = seq.fit(x, y, epochs=n_epochs, validation_split=0.1,\n",
    "                  batch_size=50, callbacks=[early_stop_callback,\n",
    "                  checkpoint_callback_mae, checkpoint_callback_loss,\n",
    "                  tb_callback, lr_callback, terminate_callback])\n",
    "#Save the trained model\n",
    "seq.save(model_path)\n",
    "\n",
    "#Save the training history\n",
    "\n",
    "np.save(f'{save_path}ConvLstm_sst0rf_rf_3bob.npy', history.history)               \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7646d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (234, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (234, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (234, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (234, 1, 25, 39)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"2017-01-01\",\"2018-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "channels = [\"FilteredrfBOB_0lag.nc\",\"FilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"FilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796e3a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 8s 568ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.62248896],\n",
       "       [0.62248896, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "pred = model.predict(x)\n",
    "\n",
    "actual = y.flatten()\n",
    "prediction = pred.flatten()\n",
    "corr = np.corrcoef(actual,prediction)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bacd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.90, 0.87, x, 0.62]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
