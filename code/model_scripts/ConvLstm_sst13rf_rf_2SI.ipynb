{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e44465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (2309, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (2309, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (2309, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (2309, 1, 25, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 15:33:26.778936: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 7, 25, 39, 4)      880       \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 7, 25, 39, 8)      3488      \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 7, 25, 39, 8)      4640      \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 7, 25, 39, 16)     13888     \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 25, 39, 16)        18496     \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 25, 39, 15)        2175      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 39, 1)         136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,703\n",
      "Trainable params: 43,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/200\n",
      "42/42 [==============================] - 303s 6s/step - loss: 1208.1577 - mae: 32.0558 - val_loss: 1061.0366 - val_mae: 31.0598 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 1020.3604 - mae: 28.8832 - val_loss: 691.5320 - val_mae: 24.3438 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 652.3798 - mae: 21.6333 - val_loss: 415.5925 - val_mae: 17.7706 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 276s 7s/step - loss: 425.3609 - mae: 15.6321 - val_loss: 248.9730 - val_mae: 12.3355 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 297.9772 - mae: 11.2517 - val_loss: 168.0980 - val_mae: 8.6688 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 238.4555 - mae: 8.9237 - val_loss: 135.7060 - val_mae: 7.1180 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 276s 7s/step - loss: 216.7313 - mae: 8.3154 - val_loss: 126.8302 - val_mae: 7.0452 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 210.6807 - mae: 8.3617 - val_loss: 125.3902 - val_mae: 7.2413 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 209.3674 - mae: 8.4800 - val_loss: 125.3448 - val_mae: 7.3775 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 209.0889 - mae: 8.5456 - val_loss: 125.3239 - val_mae: 7.4195 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 208.9364 - mae: 8.5607 - val_loss: 125.2274 - val_mae: 7.4252 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 208.8240 - mae: 8.5624 - val_loss: 125.1483 - val_mae: 7.4296 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 208.7467 - mae: 8.5713 - val_loss: 125.0029 - val_mae: 7.4107 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 208.6593 - mae: 8.5404 - val_loss: 124.9581 - val_mae: 7.4252 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 208.5682 - mae: 8.5780 - val_loss: 124.8439 - val_mae: 7.4158 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 208.4490 - mae: 8.5528 - val_loss: 124.6922 - val_mae: 7.3917 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 237s 6s/step - loss: 208.3707 - mae: 8.5348 - val_loss: 124.6537 - val_mae: 7.4068 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 208.2865 - mae: 8.5475 - val_loss: 124.5740 - val_mae: 7.4083 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 275s 7s/step - loss: 208.1915 - mae: 8.5501 - val_loss: 124.4338 - val_mae: 7.3899 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 274s 6s/step - loss: 208.0888 - mae: 8.5228 - val_loss: 124.3701 - val_mae: 7.3968 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 207.9989 - mae: 8.5501 - val_loss: 124.3293 - val_mae: 7.4110 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 207.9172 - mae: 8.5197 - val_loss: 124.1368 - val_mae: 7.3768 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 207.8108 - mae: 8.5271 - val_loss: 124.0699 - val_mae: 7.3847 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 207.7120 - mae: 8.5243 - val_loss: 124.0710 - val_mae: 7.4130 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 207.6320 - mae: 8.5573 - val_loss: 123.8739 - val_mae: 7.3789 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 286s 7s/step - loss: 207.5247 - mae: 8.5112 - val_loss: 123.7814 - val_mae: 7.3749 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 207.4329 - mae: 8.5175 - val_loss: 123.6745 - val_mae: 7.3667 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 207.3636 - mae: 8.4997 - val_loss: 123.6555 - val_mae: 7.3871 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 207.2738 - mae: 8.5206 - val_loss: 123.6188 - val_mae: 7.4011 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 207.2011 - mae: 8.4991 - val_loss: 123.4500 - val_mae: 7.3765 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 293s 7s/step - loss: 207.0786 - mae: 8.5273 - val_loss: 123.2989 - val_mae: 7.3566 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 206.9760 - mae: 8.4835 - val_loss: 123.1906 - val_mae: 7.3506 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 206.8836 - mae: 8.5038 - val_loss: 123.1538 - val_mae: 7.3679 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 206.7672 - mae: 8.4840 - val_loss: 122.9865 - val_mae: 7.3444 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 206.7053 - mae: 8.4640 - val_loss: 122.9793 - val_mae: 7.3711 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 206.5965 - mae: 8.5315 - val_loss: 122.7624 - val_mae: 7.3338 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 279s 7s/step - loss: 206.4633 - mae: 8.4560 - val_loss: 122.5909 - val_mae: 7.3079 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 206.3349 - mae: 8.4804 - val_loss: 122.6131 - val_mae: 7.3487 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 206.2453 - mae: 8.4890 - val_loss: 122.4854 - val_mae: 7.3405 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 206.1373 - mae: 8.4570 - val_loss: 122.3131 - val_mae: 7.3185 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 206.0285 - mae: 8.4755 - val_loss: 122.1749 - val_mae: 7.3072 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 205.8960 - mae: 8.4657 - val_loss: 122.0617 - val_mae: 7.3051 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 205.8501 - mae: 8.4886 - val_loss: 121.8002 - val_mae: 7.2457 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 205.6848 - mae: 8.3992 - val_loss: 121.8018 - val_mae: 7.2907 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "42/42 [==============================] - 288s 7s/step - loss: 205.5810 - mae: 8.4935 - val_loss: 121.7735 - val_mae: 7.3173 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 205.4470 - mae: 8.4429 - val_loss: 121.4956 - val_mae: 7.2617 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 205.3232 - mae: 8.4327 - val_loss: 121.3541 - val_mae: 7.2516 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 205.1742 - mae: 8.4323 - val_loss: 121.4121 - val_mae: 7.3096 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 205.0520 - mae: 8.4189 - val_loss: 121.1899 - val_mae: 7.2767 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 204.9246 - mae: 8.4489 - val_loss: 121.0269 - val_mae: 7.2620 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 204.7766 - mae: 8.4152 - val_loss: 120.9065 - val_mae: 7.2618 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 204.6516 - mae: 8.4001 - val_loss: 120.8814 - val_mae: 7.2909 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 204.5183 - mae: 8.4162 - val_loss: 120.7110 - val_mae: 7.2768 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 204.4049 - mae: 8.4022 - val_loss: 120.5949 - val_mae: 7.2785 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 204.2657 - mae: 8.4300 - val_loss: 120.2982 - val_mae: 7.2242 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 204.1195 - mae: 8.3737 - val_loss: 120.2032 - val_mae: 7.2351 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 203.9462 - mae: 8.4053 - val_loss: 120.1091 - val_mae: 7.2461 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 275s 7s/step - loss: 203.8279 - mae: 8.3624 - val_loss: 120.0398 - val_mae: 7.2630 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 203.6980 - mae: 8.3749 - val_loss: 119.8462 - val_mae: 7.2446 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 203.5179 - mae: 8.3794 - val_loss: 119.5812 - val_mae: 7.2035 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 203.3855 - mae: 8.3596 - val_loss: 119.5248 - val_mae: 7.2280 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 203.2146 - mae: 8.3942 - val_loss: 119.1802 - val_mae: 7.1575 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 203.1006 - mae: 8.3466 - val_loss: 119.0518 - val_mae: 7.1626 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 202.9220 - mae: 8.3338 - val_loss: 119.1207 - val_mae: 7.2284 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 202.7845 - mae: 8.3426 - val_loss: 118.7652 - val_mae: 7.1627 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 202.6426 - mae: 8.3554 - val_loss: 118.5483 - val_mae: 7.1361 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 202.4350 - mae: 8.3333 - val_loss: 118.4989 - val_mae: 7.1696 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 202.2616 - mae: 8.2972 - val_loss: 118.2986 - val_mae: 7.1514 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 202.0882 - mae: 8.3362 - val_loss: 118.1247 - val_mae: 7.1423 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 202.0330 - mae: 8.2940 - val_loss: 118.3527 - val_mae: 7.2423 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 201.8146 - mae: 8.2909 - val_loss: 117.9578 - val_mae: 7.1794 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 201.5863 - mae: 8.3342 - val_loss: 117.5174 - val_mae: 7.0858 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 201.4463 - mae: 8.2658 - val_loss: 117.5397 - val_mae: 7.1452 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 201.2867 - mae: 8.2904 - val_loss: 117.3423 - val_mae: 7.1316 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 201.0595 - mae: 8.2783 - val_loss: 117.0080 - val_mae: 7.0721 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 200.9720 - mae: 8.3010 - val_loss: 116.7748 - val_mae: 7.0430 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 200.9125 - mae: 8.2310 - val_loss: 116.8588 - val_mae: 7.1248 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 200.5536 - mae: 8.2474 - val_loss: 116.4597 - val_mae: 7.0479 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 200.3718 - mae: 8.2446 - val_loss: 116.4695 - val_mae: 7.1028 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 200.1879 - mae: 8.2279 - val_loss: 116.3171 - val_mae: 7.1041 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 272s 7s/step - loss: 200.0215 - mae: 8.2750 - val_loss: 115.9427 - val_mae: 7.0389 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 275s 7s/step - loss: 199.8400 - mae: 8.1681 - val_loss: 115.7510 - val_mae: 7.0287 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 199.6577 - mae: 8.2342 - val_loss: 115.5607 - val_mae: 7.0197 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 199.4521 - mae: 8.2300 - val_loss: 115.4142 - val_mae: 7.0259 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 272s 7s/step - loss: 199.2859 - mae: 8.1641 - val_loss: 115.4703 - val_mae: 7.0864 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 199.1055 - mae: 8.2181 - val_loss: 115.0194 - val_mae: 7.0036 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 198.9116 - mae: 8.1589 - val_loss: 114.8162 - val_mae: 6.9908 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 198.7186 - mae: 8.2087 - val_loss: 114.5945 - val_mae: 6.9713 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 198.5475 - mae: 8.1491 - val_loss: 114.7359 - val_mae: 7.0588 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 198.2854 - mae: 8.1837 - val_loss: 114.0565 - val_mae: 6.8833 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 285s 7s/step - loss: 198.2414 - mae: 8.1581 - val_loss: 114.0160 - val_mae: 6.9450 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 197.9536 - mae: 8.1238 - val_loss: 113.7541 - val_mae: 6.9093 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 197.7381 - mae: 8.1246 - val_loss: 113.7601 - val_mae: 6.9691 - lr: 1.0000e-04\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 197.5331 - mae: 8.1250 - val_loss: 113.4801 - val_mae: 6.9340 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 197.3723 - mae: 8.0878 - val_loss: 113.4043 - val_mae: 6.9611 - lr: 1.0000e-04\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 197.2344 - mae: 8.1652 - val_loss: 112.9638 - val_mae: 6.8698 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 196.9818 - mae: 8.0805 - val_loss: 112.8832 - val_mae: 6.9044 - lr: 1.0000e-04\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 263s 6s/step - loss: 196.8101 - mae: 8.0910 - val_loss: 112.9826 - val_mae: 6.9773 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 196.6100 - mae: 8.0772 - val_loss: 112.7189 - val_mae: 6.9516 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 196.5074 - mae: 8.1203 - val_loss: 112.3527 - val_mae: 6.8962 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 196.2157 - mae: 8.0430 - val_loss: 112.4086 - val_mae: 6.9535 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 195.9966 - mae: 8.0508 - val_loss: 111.9544 - val_mae: 6.8763 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 195.8320 - mae: 8.0672 - val_loss: 111.6280 - val_mae: 6.8232 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 195.6013 - mae: 8.0133 - val_loss: 111.6042 - val_mae: 6.8695 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 195.4898 - mae: 8.0455 - val_loss: 111.6458 - val_mae: 6.9202 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 195.2413 - mae: 8.0269 - val_loss: 111.1058 - val_mae: 6.8174 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 195.0508 - mae: 8.0321 - val_loss: 111.0448 - val_mae: 6.8480 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 194.8313 - mae: 7.9762 - val_loss: 110.9259 - val_mae: 6.8582 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 194.6580 - mae: 8.0059 - val_loss: 110.6543 - val_mae: 6.8279 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 194.4701 - mae: 8.0079 - val_loss: 110.4284 - val_mae: 6.8095 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 194.3176 - mae: 7.9814 - val_loss: 109.9452 - val_mae: 6.6876 - lr: 1.0000e-04\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 194.0854 - mae: 7.9609 - val_loss: 110.1440 - val_mae: 6.8178 - lr: 1.0000e-04\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 193.8867 - mae: 7.9811 - val_loss: 109.7140 - val_mae: 6.7379 - lr: 1.0000e-04\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 193.7019 - mae: 7.9130 - val_loss: 110.1577 - val_mae: 6.8876 - lr: 1.0000e-04\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 193.6617 - mae: 7.9782 - val_loss: 109.7740 - val_mae: 6.8371 - lr: 1.0000e-04\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 280s 7s/step - loss: 193.4355 - mae: 7.9292 - val_loss: 109.5436 - val_mae: 6.8183 - lr: 1.0000e-04\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 193.2114 - mae: 7.9238 - val_loss: 109.2220 - val_mae: 6.7769 - lr: 1.0000e-04\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 192.9816 - mae: 7.9203 - val_loss: 108.9974 - val_mae: 6.7578 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 192.8090 - mae: 7.9460 - val_loss: 108.8097 - val_mae: 6.7479 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 192.7476 - mae: 7.8732 - val_loss: 108.9131 - val_mae: 6.8047 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 192.3117 - mae: 7.9343 - val_loss: 108.0234 - val_mae: 6.5569 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 192.3199 - mae: 7.8609 - val_loss: 108.1086 - val_mae: 6.6764 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 192.1025 - mae: 7.8549 - val_loss: 107.9869 - val_mae: 6.6840 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 191.8707 - mae: 7.8787 - val_loss: 107.7252 - val_mae: 6.6495 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 191.7158 - mae: 7.8681 - val_loss: 107.3773 - val_mae: 6.5737 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 275s 7s/step - loss: 191.5980 - mae: 7.8704 - val_loss: 107.1861 - val_mae: 6.5557 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 191.3979 - mae: 7.8463 - val_loss: 107.0331 - val_mae: 6.5577 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 191.3916 - mae: 7.8368 - val_loss: 106.8000 - val_mae: 6.5081 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 191.0164 - mae: 7.7931 - val_loss: 107.0549 - val_mae: 6.6586 - lr: 1.0000e-04\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 190.8642 - mae: 7.8421 - val_loss: 106.6381 - val_mae: 6.5772 - lr: 1.0000e-04\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 190.7128 - mae: 7.8074 - val_loss: 106.7314 - val_mae: 6.6432 - lr: 1.0000e-04\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 190.5974 - mae: 7.8163 - val_loss: 106.3073 - val_mae: 6.5595 - lr: 1.0000e-04\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 190.3429 - mae: 7.7843 - val_loss: 106.1930 - val_mae: 6.5670 - lr: 1.0000e-04\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 190.2308 - mae: 7.7702 - val_loss: 105.9770 - val_mae: 6.5403 - lr: 1.0000e-04\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 190.0141 - mae: 7.8112 - val_loss: 105.7412 - val_mae: 6.5029 - lr: 1.0000e-04\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 189.9021 - mae: 7.7626 - val_loss: 105.8463 - val_mae: 6.5804 - lr: 1.0000e-04\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 189.7257 - mae: 7.7519 - val_loss: 105.4909 - val_mae: 6.5127 - lr: 1.0000e-04\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 189.6025 - mae: 7.7474 - val_loss: 105.2471 - val_mae: 6.4695 - lr: 1.0000e-04\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 189.6156 - mae: 7.7786 - val_loss: 104.9968 - val_mae: 6.3894 - lr: 1.0000e-04\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 189.3014 - mae: 7.7148 - val_loss: 105.5670 - val_mae: 6.6261 - lr: 1.0000e-04\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 189.1695 - mae: 7.7380 - val_loss: 104.9263 - val_mae: 6.4956 - lr: 1.0000e-04\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 188.9821 - mae: 7.7500 - val_loss: 104.9013 - val_mae: 6.5234 - lr: 1.0000e-04\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 188.8274 - mae: 7.6767 - val_loss: 104.7775 - val_mae: 6.5209 - lr: 1.0000e-04\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 188.7389 - mae: 7.7171 - val_loss: 104.8699 - val_mae: 6.5692 - lr: 1.0000e-04\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 188.5589 - mae: 7.7111 - val_loss: 104.2943 - val_mae: 6.4445 - lr: 1.0000e-04\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 188.4037 - mae: 7.7058 - val_loss: 104.3180 - val_mae: 6.4872 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 188.2583 - mae: 7.6911 - val_loss: 103.9318 - val_mae: 6.3940 - lr: 1.0000e-04\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 188.1427 - mae: 7.6601 - val_loss: 104.0125 - val_mae: 6.4624 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 188.0061 - mae: 7.7083 - val_loss: 103.6353 - val_mae: 6.3652 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 187.9266 - mae: 7.6527 - val_loss: 103.5733 - val_mae: 6.3904 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 187.7466 - mae: 7.6693 - val_loss: 103.5436 - val_mae: 6.4187 - lr: 1.0000e-04\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 187.6555 - mae: 7.6412 - val_loss: 103.7310 - val_mae: 6.4946 - lr: 1.0000e-04\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 187.5071 - mae: 7.7121 - val_loss: 103.0666 - val_mae: 6.2999 - lr: 1.0000e-04\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 187.4735 - mae: 7.5856 - val_loss: 103.7559 - val_mae: 6.5385 - lr: 1.0000e-04\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 187.2203 - mae: 7.6628 - val_loss: 102.9375 - val_mae: 6.3542 - lr: 1.0000e-04\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 187.2635 - mae: 7.6482 - val_loss: 102.6857 - val_mae: 6.2576 - lr: 1.0000e-04\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 187.1274 - mae: 7.6424 - val_loss: 102.5852 - val_mae: 6.2800 - lr: 1.0000e-04\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 186.9843 - mae: 7.6004 - val_loss: 102.5523 - val_mae: 6.3230 - lr: 1.0000e-04\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 186.7876 - mae: 7.6056 - val_loss: 102.6330 - val_mae: 6.3835 - lr: 1.0000e-04\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 186.6721 - mae: 7.6160 - val_loss: 102.4231 - val_mae: 6.3479 - lr: 1.0000e-04\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 186.5648 - mae: 7.6038 - val_loss: 102.3656 - val_mae: 6.3588 - lr: 1.0000e-04\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 186.4614 - mae: 7.5730 - val_loss: 102.5081 - val_mae: 6.4185 - lr: 1.0000e-04\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 186.3086 - mae: 7.6122 - val_loss: 102.1018 - val_mae: 6.3326 - lr: 1.0000e-04\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 186.2262 - mae: 7.5817 - val_loss: 101.9105 - val_mae: 6.2970 - lr: 1.0000e-04\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 186.1591 - mae: 7.5772 - val_loss: 101.8013 - val_mae: 6.2897 - lr: 1.0000e-04\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 186.0727 - mae: 7.5928 - val_loss: 101.5779 - val_mae: 6.2155 - lr: 1.0000e-04\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 186.0058 - mae: 7.5553 - val_loss: 101.5912 - val_mae: 6.2770 - lr: 1.0000e-04\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 185.8447 - mae: 7.5751 - val_loss: 101.4646 - val_mae: 6.2608 - lr: 1.0000e-04\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 279s 7s/step - loss: 185.7838 - mae: 7.5300 - val_loss: 101.4539 - val_mae: 6.2889 - lr: 1.0000e-04\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 185.6277 - mae: 7.5582 - val_loss: 101.4782 - val_mae: 6.3206 - lr: 1.0000e-04\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 185.5287 - mae: 7.5818 - val_loss: 101.2273 - val_mae: 6.2670 - lr: 1.0000e-04\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 275s 7s/step - loss: 185.4620 - mae: 7.5132 - val_loss: 101.5645 - val_mae: 6.3778 - lr: 1.0000e-04\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 185.4825 - mae: 7.6015 - val_loss: 101.1103 - val_mae: 6.2803 - lr: 1.0000e-04\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 185.4160 - mae: 7.5390 - val_loss: 100.8920 - val_mae: 6.2290 - lr: 1.0000e-04\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 185.1996 - mae: 7.4936 - val_loss: 100.9963 - val_mae: 6.2893 - lr: 1.0000e-04\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 185.1024 - mae: 7.5484 - val_loss: 100.7482 - val_mae: 6.2322 - lr: 1.0000e-04\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 184.9815 - mae: 7.5156 - val_loss: 100.7696 - val_mae: 6.2638 - lr: 1.0000e-04\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 184.9014 - mae: 7.5243 - val_loss: 100.6154 - val_mae: 6.2366 - lr: 1.0000e-04\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 184.8596 - mae: 7.5052 - val_loss: 100.7942 - val_mae: 6.3066 - lr: 1.0000e-04\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 284s 7s/step - loss: 184.7355 - mae: 7.5121 - val_loss: 100.6403 - val_mae: 6.2837 - lr: 1.0000e-04\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 292s 7s/step - loss: 184.6731 - mae: 7.5059 - val_loss: 100.4916 - val_mae: 6.2608 - lr: 1.0000e-04\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 184.5930 - mae: 7.4988 - val_loss: 100.2922 - val_mae: 6.2190 - lr: 1.0000e-04\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 184.7198 - mae: 7.5118 - val_loss: 100.4015 - val_mae: 6.2701 - lr: 1.0000e-04\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 184.4673 - mae: 7.4382 - val_loss: 101.0007 - val_mae: 6.4079 - lr: 1.0000e-04\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 184.4086 - mae: 7.5736 - val_loss: 99.8865 - val_mae: 6.0852 - lr: 1.0000e-04\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 184.4448 - mae: 7.4663 - val_loss: 99.9138 - val_mae: 6.1718 - lr: 1.0000e-04\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 184.2688 - mae: 7.4946 - val_loss: 99.8084 - val_mae: 6.1519 - lr: 1.0000e-04\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 184.1967 - mae: 7.4470 - val_loss: 100.0605 - val_mae: 6.2547 - lr: 1.0000e-04\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 184.1339 - mae: 7.5090 - val_loss: 99.7090 - val_mae: 6.1619 - lr: 1.0000e-04\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 184.0952 - mae: 7.4795 - val_loss: 99.6033 - val_mae: 6.1399 - lr: 1.0000e-04\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 183.9585 - mae: 7.4707 - val_loss: 99.7211 - val_mae: 6.2049 - lr: 1.0000e-04\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 184.0272 - mae: 7.4862 - val_loss: 99.4480 - val_mae: 6.1187 - lr: 1.0000e-04\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 183.9149 - mae: 7.4321 - val_loss: 99.6463 - val_mae: 6.2129 - lr: 1.0000e-04\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 183.7705 - mae: 7.4888 - val_loss: 99.3351 - val_mae: 6.1191 - lr: 1.0000e-04\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 183.8792 - mae: 7.4395 - val_loss: 99.2422 - val_mae: 6.0948 - lr: 1.0000e-04\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 223s 5s/step - loss: 183.6939 - mae: 7.4424 - val_loss: 99.6732 - val_mae: 6.2551 - lr: 1.0000e-04\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 218s 5s/step - loss: 183.6138 - mae: 7.4365 - val_loss: 99.5634 - val_mae: 6.2398 - lr: 1.0000e-04\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 150s 4s/step - loss: 183.5546 - mae: 7.4256 - val_loss: 99.7966 - val_mae: 6.2997 - lr: 1.0000e-04\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 70s 2s/step - loss: 183.5359 - mae: 7.4603 - val_loss: 99.2252 - val_mae: 6.1736 - lr: 1.0000e-04\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 183.4352 - mae: 7.4579 - val_loss: 99.0813 - val_mae: 6.1415 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# from sst and rainfall ---> rainfall\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Conv2D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Lambda\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2DTranspose, TimeDistributed\n",
    "from keras import callbacks\n",
    "import xarray as xr\n",
    "\n",
    "# Define global variables\n",
    "lead_time = 2\n",
    "save_path = \"/home/cccr/roxy/matin/MTech_project/model/Conv-LSTM/7in3out/\"\n",
    "model_path = save_path + \"ConvLstm_sst13rf_rf_2SI.h5\"\n",
    "add_data = \"/home/cccr/roxy/matin/MTech_project/data/\"\n",
    "\n",
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"1998-01-01\",\"2016-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_6lag.nc\",\"SIFilteredSSTBOB_6.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_6lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n",
    "# Perform additional processing or modeling steps as needed\n",
    "seq = tf.keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "seq.add(ConvLSTM2D(filters=4, kernel_size=(3,3), padding='same', input_shape=(7,25,39,2), return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=False, data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=15, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "# Create the query, value, and key inputs\n",
    "query_input = Input(shape=(25,39,2), name='query_input')\n",
    "value_input = Input(shape=(25,39,2), name='value_input')\n",
    "key_input = Input(shape=(25,39,2), name='key_input')\n",
    "\n",
    "# Pass the inputs to the Attention layer\n",
    "attention_output = Attention(name='attention')([query_input, value_input, key_input])\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=10**-4)\n",
    "seq.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae'])\n",
    "\n",
    "print(seq.summary())\n",
    "\n",
    "# Define the number of training epochs\n",
    "n_epochs = 200\n",
    "logdir = save_path\n",
    "# Define the TensorBoard callback for monitoring training progress\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=10, batch_size=10, write_graph=True)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback for reducing the learning rate when the model plateaus\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=10**-20)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation mean absolute error\n",
    "checkpoint_callback_mae = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_mae',\n",
    "                                                             mode='min', save_best_only=True)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation loss\n",
    "checkpoint_callback_loss = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss',\n",
    "                                                              mode='min', save_best_only=True)\n",
    "\n",
    "# Define the EarlyStopping callback for stopping training when the model stops improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "# Define the TerminateOnNaN callback for terminating the training process if there is NaN value in any of the output\n",
    "terminate_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "# Train the model using the defined callbacks\n",
    "history = seq.fit(x, y, epochs=n_epochs, validation_split=0.1,\n",
    "                  batch_size=50, callbacks=[early_stop_callback,\n",
    "                  checkpoint_callback_mae, checkpoint_callback_loss,\n",
    "                  tb_callback, lr_callback, terminate_callback])\n",
    "#Save the trained model\n",
    "seq.save(model_path)\n",
    "\n",
    "#Save the training history\n",
    "\n",
    "np.save(f'{save_path}ConvLstm_sst13rf_rf_2SI.npy', history.history)               \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f01f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (235, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (235, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (235, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (235, 1, 25, 39)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"2017-01-01\",\"2018-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_6lag.nc\",\"SIFilteredSSTBOB_6.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_6lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7380b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 85ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01709859],\n",
       "       [0.01709859, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "pred = model.predict(x)\n",
    "\n",
    "actual = y.flatten()\n",
    "prediction = pred.flatten()\n",
    "corr = np.corrcoef(actual,prediction)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
