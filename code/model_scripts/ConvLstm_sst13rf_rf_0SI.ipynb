{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6fd901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (2311, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (2311, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (2311, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (2311, 1, 25, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 15:33:01.339021: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 7, 25, 39, 4)      880       \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 7, 25, 39, 8)      3488      \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 7, 25, 39, 8)      4640      \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 7, 25, 39, 16)     13888     \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 25, 39, 16)        18496     \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 25, 39, 15)        2175      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 25, 39, 1)         136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,703\n",
      "Trainable params: 43,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/200\n",
      "42/42 [==============================] - 258s 5s/step - loss: 1215.2415 - mae: 32.1707 - val_loss: 1099.5118 - val_mae: 31.6473 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 1126.0931 - mae: 30.7297 - val_loss: 789.2340 - val_mae: 26.2502 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 658.2944 - mae: 21.7423 - val_loss: 410.5655 - val_mae: 17.6165 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 421.4581 - mae: 15.5739 - val_loss: 251.2717 - val_mae: 12.4398 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 294.1418 - mae: 11.1840 - val_loss: 164.9881 - val_mae: 8.5492 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 273s 7s/step - loss: 231.4275 - mae: 8.7266 - val_loss: 131.6300 - val_mae: 6.9766 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 210.2504 - mae: 8.1674 - val_loss: 123.8903 - val_mae: 7.0090 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 205.6623 - mae: 8.2891 - val_loss: 123.1394 - val_mae: 7.2031 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 204.8728 - mae: 8.3762 - val_loss: 123.1052 - val_mae: 7.2829 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 204.7102 - mae: 8.4267 - val_loss: 123.0794 - val_mae: 7.3183 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 204.5782 - mae: 8.4278 - val_loss: 122.9172 - val_mae: 7.2972 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 204.4487 - mae: 8.4164 - val_loss: 122.8417 - val_mae: 7.3050 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 204.3519 - mae: 8.4212 - val_loss: 122.7571 - val_mae: 7.3059 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 204.2691 - mae: 8.4202 - val_loss: 122.6363 - val_mae: 7.2932 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 204.1616 - mae: 8.4050 - val_loss: 122.5382 - val_mae: 7.2896 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 204.0766 - mae: 8.3903 - val_loss: 122.4356 - val_mae: 7.2848 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 203.9690 - mae: 8.4172 - val_loss: 122.3691 - val_mae: 7.2942 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 203.8582 - mae: 8.4000 - val_loss: 122.2153 - val_mae: 7.2721 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 203.7641 - mae: 8.3941 - val_loss: 122.0773 - val_mae: 7.2556 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 203.6739 - mae: 8.3958 - val_loss: 122.0136 - val_mae: 7.2698 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 286s 7s/step - loss: 203.5375 - mae: 8.3835 - val_loss: 121.9482 - val_mae: 7.2821 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 203.4243 - mae: 8.4008 - val_loss: 121.7687 - val_mae: 7.2542 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 203.3046 - mae: 8.3512 - val_loss: 121.6034 - val_mae: 7.2298 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 273s 7s/step - loss: 203.1944 - mae: 8.3648 - val_loss: 121.5662 - val_mae: 7.2575 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 203.0553 - mae: 8.3682 - val_loss: 121.4375 - val_mae: 7.2503 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 282s 7s/step - loss: 203.0084 - mae: 8.3960 - val_loss: 121.2132 - val_mae: 7.2052 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 202.8243 - mae: 8.3265 - val_loss: 121.1779 - val_mae: 7.2374 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 202.6918 - mae: 8.3629 - val_loss: 121.0582 - val_mae: 7.2362 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 202.5517 - mae: 8.3468 - val_loss: 120.8820 - val_mae: 7.2146 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 202.4336 - mae: 8.3280 - val_loss: 120.7212 - val_mae: 7.1988 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 202.2925 - mae: 8.3412 - val_loss: 120.6471 - val_mae: 7.2177 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 202.1589 - mae: 8.3150 - val_loss: 120.4957 - val_mae: 7.2079 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 202.0276 - mae: 8.3321 - val_loss: 120.3776 - val_mae: 7.2110 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 201.8786 - mae: 8.3156 - val_loss: 120.2149 - val_mae: 7.1990 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 201.7339 - mae: 8.3359 - val_loss: 120.0430 - val_mae: 7.1912 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 201.3523 - mae: 8.2576 - val_loss: 119.3853 - val_mae: 7.1698 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 200.8120 - mae: 8.2798 - val_loss: 118.9997 - val_mae: 7.1668 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 200.4312 - mae: 8.2893 - val_loss: 118.5700 - val_mae: 7.1300 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 200.1159 - mae: 8.2488 - val_loss: 118.0570 - val_mae: 7.0439 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 199.7368 - mae: 8.1859 - val_loss: 118.0710 - val_mae: 7.1419 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 275s 7s/step - loss: 199.4619 - mae: 8.2603 - val_loss: 117.5721 - val_mae: 7.0846 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 199.2056 - mae: 8.1785 - val_loss: 117.5056 - val_mae: 7.1216 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 198.8652 - mae: 8.1951 - val_loss: 116.9127 - val_mae: 7.0267 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 198.6008 - mae: 8.1784 - val_loss: 116.6270 - val_mae: 7.0163 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 198.2991 - mae: 8.1478 - val_loss: 116.3087 - val_mae: 6.9950 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 197.9850 - mae: 8.1624 - val_loss: 116.1697 - val_mae: 7.0333 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 197.7387 - mae: 8.1438 - val_loss: 115.8455 - val_mae: 7.0118 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 197.4694 - mae: 8.1221 - val_loss: 115.6469 - val_mae: 7.0093 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 197.1892 - mae: 8.1433 - val_loss: 115.2603 - val_mae: 6.9764 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 196.9075 - mae: 8.0950 - val_loss: 114.8906 - val_mae: 6.9276 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 196.6494 - mae: 8.0726 - val_loss: 114.7154 - val_mae: 6.9513 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 196.3842 - mae: 8.0577 - val_loss: 114.3441 - val_mae: 6.9069 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 274s 7s/step - loss: 196.1630 - mae: 8.1133 - val_loss: 114.1925 - val_mae: 6.9332 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 195.9188 - mae: 8.0282 - val_loss: 113.8428 - val_mae: 6.8930 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 195.6346 - mae: 8.0530 - val_loss: 113.5374 - val_mae: 6.8707 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 195.3301 - mae: 8.0234 - val_loss: 113.5995 - val_mae: 6.9544 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 195.1125 - mae: 8.0453 - val_loss: 113.0900 - val_mae: 6.8742 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 194.7951 - mae: 8.0009 - val_loss: 112.8047 - val_mae: 6.8557 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 194.5218 - mae: 8.0120 - val_loss: 112.5267 - val_mae: 6.8355 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 194.3475 - mae: 7.9691 - val_loss: 112.6372 - val_mae: 6.9180 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 194.0303 - mae: 7.9910 - val_loss: 111.9704 - val_mae: 6.7924 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 193.8295 - mae: 7.9700 - val_loss: 111.6421 - val_mae: 6.7283 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 193.6270 - mae: 7.9281 - val_loss: 111.7622 - val_mae: 6.8461 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 193.3789 - mae: 7.9385 - val_loss: 111.3015 - val_mae: 6.7761 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 193.0685 - mae: 7.9384 - val_loss: 111.0369 - val_mae: 6.7669 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 192.8545 - mae: 7.9110 - val_loss: 110.7843 - val_mae: 6.7543 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 192.6608 - mae: 7.9444 - val_loss: 110.4182 - val_mae: 6.6761 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 192.3965 - mae: 7.9143 - val_loss: 110.1045 - val_mae: 6.6431 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 192.1780 - mae: 7.8199 - val_loss: 110.1228 - val_mae: 6.7365 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 191.8552 - mae: 7.8964 - val_loss: 109.8974 - val_mae: 6.7279 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 191.6734 - mae: 7.8596 - val_loss: 109.6443 - val_mae: 6.7040 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 191.4139 - mae: 7.8653 - val_loss: 109.3521 - val_mae: 6.6722 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 191.2297 - mae: 7.8074 - val_loss: 109.5511 - val_mae: 6.7799 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 191.0227 - mae: 7.8783 - val_loss: 108.7715 - val_mae: 6.6060 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 190.7707 - mae: 7.8171 - val_loss: 108.5449 - val_mae: 6.5933 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 190.6111 - mae: 7.7814 - val_loss: 108.5274 - val_mae: 6.6538 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 190.3076 - mae: 7.8510 - val_loss: 108.1605 - val_mae: 6.5928 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 190.0740 - mae: 7.7703 - val_loss: 107.9952 - val_mae: 6.6000 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 276s 7s/step - loss: 189.9414 - mae: 7.7795 - val_loss: 107.7529 - val_mae: 6.5774 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 189.6669 - mae: 7.7693 - val_loss: 107.7216 - val_mae: 6.6228 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 189.5027 - mae: 7.7624 - val_loss: 107.4036 - val_mae: 6.5797 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 189.2630 - mae: 7.7287 - val_loss: 107.3112 - val_mae: 6.6027 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 286s 7s/step - loss: 189.1167 - mae: 7.7546 - val_loss: 107.1295 - val_mae: 6.5935 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 188.9050 - mae: 7.7301 - val_loss: 106.9301 - val_mae: 6.5855 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 188.6848 - mae: 7.7046 - val_loss: 106.7519 - val_mae: 6.5713 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 188.6700 - mae: 7.7405 - val_loss: 106.2984 - val_mae: 6.4443 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 188.3590 - mae: 7.7085 - val_loss: 106.1703 - val_mae: 6.4971 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 188.1249 - mae: 7.6965 - val_loss: 105.9793 - val_mae: 6.4874 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 187.9919 - mae: 7.6514 - val_loss: 105.8815 - val_mae: 6.5085 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 187.7463 - mae: 7.6676 - val_loss: 105.7226 - val_mae: 6.5032 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 187.5902 - mae: 7.6692 - val_loss: 105.5500 - val_mae: 6.4972 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 187.4105 - mae: 7.6459 - val_loss: 105.4920 - val_mae: 6.5117 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 187.2611 - mae: 7.6548 - val_loss: 105.0653 - val_mae: 6.4355 - lr: 1.0000e-04\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 187.0715 - mae: 7.6527 - val_loss: 104.9202 - val_mae: 6.4354 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 186.9614 - mae: 7.6310 - val_loss: 104.6916 - val_mae: 6.3921 - lr: 1.0000e-04\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 186.6911 - mae: 7.5978 - val_loss: 104.8622 - val_mae: 6.4913 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 186.5811 - mae: 7.6047 - val_loss: 104.4277 - val_mae: 6.4104 - lr: 1.0000e-04\n",
      "Epoch 98/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 257s 6s/step - loss: 186.3795 - mae: 7.6521 - val_loss: 104.1056 - val_mae: 6.3367 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 186.2611 - mae: 7.5667 - val_loss: 104.1292 - val_mae: 6.3986 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 186.0610 - mae: 7.5947 - val_loss: 103.8560 - val_mae: 6.3464 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 282s 7s/step - loss: 185.9455 - mae: 7.5657 - val_loss: 103.7571 - val_mae: 6.3561 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 185.7513 - mae: 7.5832 - val_loss: 103.5959 - val_mae: 6.3486 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 185.6572 - mae: 7.5492 - val_loss: 103.4639 - val_mae: 6.3448 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 185.5131 - mae: 7.5732 - val_loss: 103.4107 - val_mae: 6.3655 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 185.3836 - mae: 7.5517 - val_loss: 103.1204 - val_mae: 6.3085 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 185.1926 - mae: 7.5231 - val_loss: 103.1313 - val_mae: 6.3510 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 185.1189 - mae: 7.5305 - val_loss: 103.2505 - val_mae: 6.4058 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 184.9553 - mae: 7.5452 - val_loss: 102.7044 - val_mae: 6.2744 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 184.8729 - mae: 7.5162 - val_loss: 102.5393 - val_mae: 6.2506 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 184.7017 - mae: 7.5057 - val_loss: 102.6634 - val_mae: 6.3365 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 184.6132 - mae: 7.5084 - val_loss: 102.5365 - val_mae: 6.3292 - lr: 1.0000e-04\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 184.4453 - mae: 7.5227 - val_loss: 102.2836 - val_mae: 6.2846 - lr: 1.0000e-04\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 184.3548 - mae: 7.4902 - val_loss: 102.0044 - val_mae: 6.1983 - lr: 1.0000e-04\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 184.2040 - mae: 7.4511 - val_loss: 102.4243 - val_mae: 6.3691 - lr: 1.0000e-04\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 184.0939 - mae: 7.4866 - val_loss: 101.8847 - val_mae: 6.2456 - lr: 1.0000e-04\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 184.0110 - mae: 7.4666 - val_loss: 101.8422 - val_mae: 6.2642 - lr: 1.0000e-04\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 183.8824 - mae: 7.4854 - val_loss: 101.9363 - val_mae: 6.3117 - lr: 1.0000e-04\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 183.8095 - mae: 7.4541 - val_loss: 101.8208 - val_mae: 6.3039 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 183.7449 - mae: 7.4666 - val_loss: 101.4292 - val_mae: 6.1959 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 246s 6s/step - loss: 183.6194 - mae: 7.4464 - val_loss: 101.8363 - val_mae: 6.3423 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 241s 6s/step - loss: 183.4888 - mae: 7.4429 - val_loss: 101.3570 - val_mae: 6.2430 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 183.4174 - mae: 7.4591 - val_loss: 101.1959 - val_mae: 6.2116 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 183.2899 - mae: 7.4356 - val_loss: 101.2174 - val_mae: 6.2466 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 242s 6s/step - loss: 183.2130 - mae: 7.4259 - val_loss: 101.0371 - val_mae: 6.2143 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 237s 6s/step - loss: 183.1160 - mae: 7.4334 - val_loss: 100.9057 - val_mae: 6.1928 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 238s 6s/step - loss: 183.0567 - mae: 7.3908 - val_loss: 101.0863 - val_mae: 6.2652 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 182.9822 - mae: 7.4419 - val_loss: 100.7249 - val_mae: 6.1685 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 182.7016 - mae: 7.4204 - val_loss: 100.4133 - val_mae: 6.1132 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 179.3852 - mae: 7.1992 - val_loss: 101.0703 - val_mae: 6.3838 - lr: 1.0000e-04\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 174.7391 - mae: 7.1476 - val_loss: 99.9743 - val_mae: 6.3844 - lr: 1.0000e-04\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 265s 6s/step - loss: 171.2278 - mae: 7.0614 - val_loss: 97.6657 - val_mae: 6.2813 - lr: 1.0000e-04\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 165.9531 - mae: 6.9417 - val_loss: 93.9488 - val_mae: 6.2160 - lr: 1.0000e-04\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 237s 6s/step - loss: 160.4784 - mae: 6.8157 - val_loss: 89.2172 - val_mae: 5.9875 - lr: 1.0000e-04\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 244s 6s/step - loss: 155.7127 - mae: 6.7198 - val_loss: 82.4985 - val_mae: 5.8011 - lr: 1.0000e-04\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 150.5093 - mae: 6.5586 - val_loss: 77.6689 - val_mae: 5.7077 - lr: 1.0000e-04\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 142.8921 - mae: 6.3464 - val_loss: 73.5416 - val_mae: 5.6886 - lr: 1.0000e-04\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 249s 6s/step - loss: 135.5475 - mae: 6.1602 - val_loss: 69.1569 - val_mae: 5.4923 - lr: 1.0000e-04\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 250s 6s/step - loss: 129.5621 - mae: 6.0368 - val_loss: 64.1469 - val_mae: 5.3827 - lr: 1.0000e-04\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 283s 7s/step - loss: 122.7668 - mae: 5.8385 - val_loss: 57.1640 - val_mae: 4.9317 - lr: 1.0000e-04\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 116.5393 - mae: 5.6506 - val_loss: 53.0436 - val_mae: 4.8954 - lr: 1.0000e-04\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 112.8258 - mae: 5.5887 - val_loss: 49.4777 - val_mae: 4.7021 - lr: 1.0000e-04\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 254s 6s/step - loss: 105.9424 - mae: 5.3527 - val_loss: 46.2073 - val_mae: 4.5995 - lr: 1.0000e-04\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 285s 7s/step - loss: 100.9602 - mae: 5.1804 - val_loss: 44.0363 - val_mae: 4.4688 - lr: 1.0000e-04\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 96.7886 - mae: 5.0441 - val_loss: 42.4603 - val_mae: 4.3578 - lr: 1.0000e-04\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 248s 6s/step - loss: 92.4411 - mae: 4.8754 - val_loss: 38.4195 - val_mae: 4.2131 - lr: 1.0000e-04\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 88.3421 - mae: 4.7207 - val_loss: 35.7767 - val_mae: 4.0091 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 259s 6s/step - loss: 83.6787 - mae: 4.5109 - val_loss: 33.0073 - val_mae: 3.8572 - lr: 1.0000e-04\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 257s 6s/step - loss: 79.8400 - mae: 4.3735 - val_loss: 30.9644 - val_mae: 3.7259 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 76.8747 - mae: 4.2610 - val_loss: 30.4647 - val_mae: 3.7075 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 74.5710 - mae: 4.1911 - val_loss: 28.8565 - val_mae: 3.5685 - lr: 1.0000e-04\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 260s 6s/step - loss: 71.8890 - mae: 4.0702 - val_loss: 28.1063 - val_mae: 3.5377 - lr: 1.0000e-04\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 270s 6s/step - loss: 69.8788 - mae: 3.9917 - val_loss: 27.6270 - val_mae: 3.4949 - lr: 1.0000e-04\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 68.5508 - mae: 3.9655 - val_loss: 27.3741 - val_mae: 3.5000 - lr: 1.0000e-04\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 66.7168 - mae: 3.8860 - val_loss: 27.8751 - val_mae: 3.4903 - lr: 1.0000e-04\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 65.3598 - mae: 3.8414 - val_loss: 26.0824 - val_mae: 3.3655 - lr: 1.0000e-04\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 63.7074 - mae: 3.7573 - val_loss: 25.1815 - val_mae: 3.2444 - lr: 1.0000e-04\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 61.5381 - mae: 3.6149 - val_loss: 22.8114 - val_mae: 3.0369 - lr: 1.0000e-04\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 243s 6s/step - loss: 59.8103 - mae: 3.5279 - val_loss: 22.5607 - val_mae: 3.0277 - lr: 1.0000e-04\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 58.1363 - mae: 3.4583 - val_loss: 22.6189 - val_mae: 3.0695 - lr: 1.0000e-04\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 237s 6s/step - loss: 56.8982 - mae: 3.4131 - val_loss: 22.2696 - val_mae: 2.9898 - lr: 1.0000e-04\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 247s 6s/step - loss: 55.3616 - mae: 3.3539 - val_loss: 21.4672 - val_mae: 2.9296 - lr: 1.0000e-04\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 54.1431 - mae: 3.3025 - val_loss: 20.5886 - val_mae: 2.8917 - lr: 1.0000e-04\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 240s 6s/step - loss: 52.9419 - mae: 3.2578 - val_loss: 20.4411 - val_mae: 2.8625 - lr: 1.0000e-04\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 253s 6s/step - loss: 52.0912 - mae: 3.2437 - val_loss: 19.4767 - val_mae: 2.8641 - lr: 1.0000e-04\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 50.9639 - mae: 3.1711 - val_loss: 18.7607 - val_mae: 2.7266 - lr: 1.0000e-04\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 50.1953 - mae: 3.1573 - val_loss: 19.2507 - val_mae: 2.7652 - lr: 1.0000e-04\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 256s 6s/step - loss: 49.1503 - mae: 3.1078 - val_loss: 21.7167 - val_mae: 3.0342 - lr: 1.0000e-04\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 267s 6s/step - loss: 48.6765 - mae: 3.1110 - val_loss: 17.9111 - val_mae: 2.6473 - lr: 1.0000e-04\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 279s 7s/step - loss: 47.5574 - mae: 3.0520 - val_loss: 17.6406 - val_mae: 2.6494 - lr: 1.0000e-04\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 273s 7s/step - loss: 46.8853 - mae: 3.0256 - val_loss: 18.2206 - val_mae: 2.7050 - lr: 1.0000e-04\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 260s 6s/step - loss: 46.0358 - mae: 2.9834 - val_loss: 17.8401 - val_mae: 2.6099 - lr: 1.0000e-04\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 255s 6s/step - loss: 45.4482 - mae: 2.9676 - val_loss: 17.0700 - val_mae: 2.5596 - lr: 1.0000e-04\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 44.7395 - mae: 2.9363 - val_loss: 16.8259 - val_mae: 2.5439 - lr: 1.0000e-04\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 296s 7s/step - loss: 44.0983 - mae: 2.9172 - val_loss: 16.4689 - val_mae: 2.5024 - lr: 1.0000e-04\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 268s 6s/step - loss: 43.5113 - mae: 2.8963 - val_loss: 16.3637 - val_mae: 2.5141 - lr: 1.0000e-04\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 271s 6s/step - loss: 43.1455 - mae: 2.8962 - val_loss: 16.2345 - val_mae: 2.4810 - lr: 1.0000e-04\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 42.5977 - mae: 2.8716 - val_loss: 15.9771 - val_mae: 2.4738 - lr: 1.0000e-04\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 263s 6s/step - loss: 42.2577 - mae: 2.8788 - val_loss: 15.8009 - val_mae: 2.4287 - lr: 1.0000e-04\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 41.7101 - mae: 2.8519 - val_loss: 15.7872 - val_mae: 2.4197 - lr: 1.0000e-04\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 41.0825 - mae: 2.8277 - val_loss: 17.4231 - val_mae: 2.6158 - lr: 1.0000e-04\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 278s 7s/step - loss: 40.8999 - mae: 2.8353 - val_loss: 17.1086 - val_mae: 2.4524 - lr: 1.0000e-04\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 328s 8s/step - loss: 40.1461 - mae: 2.7818 - val_loss: 15.4005 - val_mae: 2.4066 - lr: 1.0000e-04\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 289s 7s/step - loss: 39.7190 - mae: 2.7662 - val_loss: 16.1106 - val_mae: 2.4363 - lr: 1.0000e-04\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 280s 7s/step - loss: 39.2441 - mae: 2.7534 - val_loss: 15.7063 - val_mae: 2.4303 - lr: 1.0000e-04\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 272s 6s/step - loss: 38.9311 - mae: 2.7434 - val_loss: 17.4876 - val_mae: 2.6036 - lr: 1.0000e-04\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 245s 6s/step - loss: 38.7749 - mae: 2.7532 - val_loss: 17.4364 - val_mae: 2.6817 - lr: 1.0000e-04\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 264s 6s/step - loss: 38.7209 - mae: 2.7882 - val_loss: 15.1246 - val_mae: 2.3258 - lr: 1.0000e-04\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 38.0125 - mae: 2.7265 - val_loss: 14.8175 - val_mae: 2.3241 - lr: 1.0000e-04\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 252s 6s/step - loss: 37.5099 - mae: 2.6995 - val_loss: 14.6788 - val_mae: 2.3041 - lr: 1.0000e-04\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 251s 6s/step - loss: 37.2999 - mae: 2.6976 - val_loss: 14.5876 - val_mae: 2.2977 - lr: 1.0000e-04\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 266s 6s/step - loss: 37.0287 - mae: 2.6998 - val_loss: 14.9488 - val_mae: 2.3697 - lr: 1.0000e-04\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 262s 6s/step - loss: 36.6699 - mae: 2.6836 - val_loss: 16.8110 - val_mae: 2.5180 - lr: 1.0000e-04\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 261s 6s/step - loss: 36.2137 - mae: 2.6601 - val_loss: 14.3716 - val_mae: 2.2714 - lr: 1.0000e-04\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 258s 6s/step - loss: 35.8724 - mae: 2.6417 - val_loss: 14.3403 - val_mae: 2.2929 - lr: 1.0000e-04\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 269s 6s/step - loss: 36.0286 - mae: 2.6943 - val_loss: 14.0193 - val_mae: 2.2496 - lr: 1.0000e-04\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 288s 7s/step - loss: 35.3768 - mae: 2.6361 - val_loss: 14.7624 - val_mae: 2.2947 - lr: 1.0000e-04\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 236s 6s/step - loss: 35.0247 - mae: 2.6184 - val_loss: 14.0614 - val_mae: 2.2753 - lr: 1.0000e-04\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 217s 5s/step - loss: 34.8222 - mae: 2.6188 - val_loss: 14.0397 - val_mae: 2.2432 - lr: 1.0000e-04\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 150s 4s/step - loss: 34.4305 - mae: 2.5935 - val_loss: 13.6070 - val_mae: 2.2149 - lr: 1.0000e-04\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 84s 2s/step - loss: 34.2309 - mae: 2.6015 - val_loss: 13.7927 - val_mae: 2.2485 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# from sst and rainfall ---> rainfall\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Conv2D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Lambda\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2DTranspose, TimeDistributed\n",
    "from keras import callbacks\n",
    "import xarray as xr\n",
    "\n",
    "# Define global variables\n",
    "lead_time = 0\n",
    "save_path = \"/home/cccr/roxy/matin/MTech_project/model/Conv-LSTM/7in1out/\"\n",
    "model_path = save_path + \"ConvLstm_sst13rf_rf_0SI.h5\"\n",
    "add_data = \"/home/cccr/roxy/matin/MTech_project/data/\"\n",
    "\n",
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"1998-01-01\",\"2016-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_6lag.nc\",\"SIFilteredSSTBOB_6.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_6lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n",
    "# Perform additional processing or modeling steps as needed\n",
    "seq = tf.keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "seq.add(ConvLSTM2D(filters=4, kernel_size=(3,3), padding='same', input_shape=(7,25,39,2), return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=False, data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=15, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "# Create the query, value, and key inputs\n",
    "query_input = Input(shape=(25,39,2), name='query_input')\n",
    "value_input = Input(shape=(25,39,2), name='value_input')\n",
    "key_input = Input(shape=(25,39,2), name='key_input')\n",
    "\n",
    "# Pass the inputs to the Attention layer\n",
    "attention_output = Attention(name='attention')([query_input, value_input, key_input])\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=10**-4)\n",
    "seq.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae'])\n",
    "\n",
    "print(seq.summary())\n",
    "\n",
    "# Define the number of training epochs\n",
    "n_epochs = 200\n",
    "logdir = save_path\n",
    "# Define the TensorBoard callback for monitoring training progress\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=10, batch_size=10, write_graph=True)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback for reducing the learning rate when the model plateaus\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=10**-20)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation mean absolute error\n",
    "checkpoint_callback_mae = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_mae',\n",
    "                                                             mode='min', save_best_only=True)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation loss\n",
    "checkpoint_callback_loss = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss',\n",
    "                                                              mode='min', save_best_only=True)\n",
    "\n",
    "# Define the EarlyStopping callback for stopping training when the model stops improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "# Define the TerminateOnNaN callback for terminating the training process if there is NaN value in any of the output\n",
    "terminate_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "# Train the model using the defined callbacks\n",
    "history = seq.fit(x, y, epochs=n_epochs, validation_split=0.1,\n",
    "                  batch_size=50, callbacks=[early_stop_callback,\n",
    "                  checkpoint_callback_mae, checkpoint_callback_loss,\n",
    "                  tb_callback, lr_callback, terminate_callback])\n",
    "#Save the trained model\n",
    "seq.save(model_path)\n",
    "\n",
    "#Save the training history\n",
    "\n",
    "np.save(f'{save_path}ConvLstm_sst13rf_rf_0SI.npy', history.history)               \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0375db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (237, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (237, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (237, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (237, 1, 25, 39)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"2017-01-01\",\"2018-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "channels = [\"SIFilteredrfBOB_6lag.nc\",\"SIFilteredSSTBOB_6.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"SIFilteredrfBOB_6lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b55ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 3s 239ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.86240535],\n",
       "       [0.86240535, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "pred = model.predict(x)\n",
    "\n",
    "actual = y.flatten()\n",
    "prediction = pred.flatten()\n",
    "corr = np.corrcoef(actual,prediction)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
