{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3144d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (2309, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (2309, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (2309, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (2309, 1, 25, 39)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, 7, 25, 39, 4)      880       \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, 7, 25, 39, 8)      3488      \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 7, 25, 39, 8)      4640      \n",
      "                                                                 \n",
      " conv_lstm2d_8 (ConvLSTM2D)  (None, 7, 25, 39, 16)     13888     \n",
      "                                                                 \n",
      " conv_lstm2d_9 (ConvLSTM2D)  (None, 25, 39, 16)        18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 39, 15)        2175      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 25, 39, 1)         136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,703\n",
      "Trainable params: 43,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/200\n",
      "42/42 [==============================] - 67s 1s/step - loss: 2575.2556 - mae: 39.6170 - val_loss: 2397.9900 - val_mae: 38.8744 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 42s 1s/step - loss: 2448.7363 - mae: 37.9604 - val_loss: 2067.8743 - val_mae: 34.3378 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 42s 1s/step - loss: 2052.3645 - mae: 32.3517 - val_loss: 1739.4557 - val_mae: 29.2781 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 42s 1s/step - loss: 1754.9623 - mae: 27.8283 - val_loss: 1474.2867 - val_mae: 24.8449 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1501.0828 - mae: 24.0415 - val_loss: 1250.8583 - val_mae: 21.1880 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 41s 978ms/step - loss: 1295.7321 - mae: 21.3778 - val_loss: 1084.6233 - val_mae: 18.9440 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 42s 1s/step - loss: 1155.5825 - mae: 20.1994 - val_loss: 984.9312 - val_mae: 18.1945 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1079.2291 - mae: 20.0309 - val_loss: 943.7941 - val_mae: 18.4108 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 46s 1s/step - loss: 1050.8397 - mae: 20.4169 - val_loss: 932.8392 - val_mae: 18.8355 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 45s 1s/step - loss: 1043.9728 - mae: 20.7603 - val_loss: 931.4551 - val_mae: 19.0883 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1042.6312 - mae: 20.9316 - val_loss: 931.4528 - val_mae: 19.2122 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1042.2645 - mae: 20.9736 - val_loss: 931.3691 - val_mae: 19.2435 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 45s 1s/step - loss: 1042.1046 - mae: 21.0184 - val_loss: 931.2438 - val_mae: 19.2599 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1041.9457 - mae: 21.0453 - val_loss: 931.1815 - val_mae: 19.2875 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 45s 1s/step - loss: 1042.0183 - mae: 20.9650 - val_loss: 930.6994 - val_mae: 19.2275 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 45s 1s/step - loss: 1041.5695 - mae: 21.0300 - val_loss: 930.7818 - val_mae: 19.2880 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 45s 1s/step - loss: 1041.5027 - mae: 21.0374 - val_loss: 930.3652 - val_mae: 19.2468 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1041.2904 - mae: 21.0148 - val_loss: 930.0752 - val_mae: 19.2296 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1041.1469 - mae: 20.9535 - val_loss: 929.8920 - val_mae: 19.2363 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1041.0402 - mae: 21.0020 - val_loss: 929.8175 - val_mae: 19.2646 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1040.7450 - mae: 21.0004 - val_loss: 929.4982 - val_mae: 19.2448 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1040.7148 - mae: 21.0049 - val_loss: 929.1494 - val_mae: 19.2135 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1040.4346 - mae: 20.9664 - val_loss: 928.8636 - val_mae: 19.1969 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1040.2375 - mae: 20.9623 - val_loss: 928.7611 - val_mae: 19.2227 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 46s 1s/step - loss: 1040.1830 - mae: 20.9518 - val_loss: 928.4991 - val_mae: 19.2121 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1040.0244 - mae: 21.0085 - val_loss: 928.5112 - val_mae: 19.2578 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1039.8678 - mae: 20.9748 - val_loss: 928.1508 - val_mae: 19.2292 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1039.7820 - mae: 20.9733 - val_loss: 927.6743 - val_mae: 19.1688 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1039.4188 - mae: 20.9422 - val_loss: 927.7636 - val_mae: 19.2370 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1039.3793 - mae: 20.9700 - val_loss: 927.4165 - val_mae: 19.2115 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1039.1191 - mae: 20.9795 - val_loss: 927.5267 - val_mae: 19.2694 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1038.9576 - mae: 20.9957 - val_loss: 927.0061 - val_mae: 19.2141 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1038.8429 - mae: 20.9753 - val_loss: 926.7150 - val_mae: 19.1994 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 46s 1s/step - loss: 1038.6069 - mae: 20.9217 - val_loss: 926.3111 - val_mae: 19.1534 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1038.5154 - mae: 20.9060 - val_loss: 926.1899 - val_mae: 19.1775 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 42s 997ms/step - loss: 1038.3799 - mae: 20.9693 - val_loss: 925.9333 - val_mae: 19.1674 - lr: 1.0000e-04\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1038.1663 - mae: 20.9050 - val_loss: 925.7155 - val_mae: 19.1664 - lr: 1.0000e-04\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1037.9653 - mae: 20.9094 - val_loss: 925.5081 - val_mae: 19.1687 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 41s 979ms/step - loss: 1037.8458 - mae: 20.9620 - val_loss: 925.2682 - val_mae: 19.1625 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 41s 978ms/step - loss: 1037.6162 - mae: 20.9220 - val_loss: 924.9974 - val_mae: 19.1496 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 42s 994ms/step - loss: 1037.5823 - mae: 20.9299 - val_loss: 924.7899 - val_mae: 19.1518 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 47s 1s/step - loss: 1037.3226 - mae: 20.9099 - val_loss: 924.7994 - val_mae: 19.2000 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200\n",
      "42/42 [==============================] - 42s 1s/step - loss: 1037.1919 - mae: 20.9549 - val_loss: 924.4783 - val_mae: 19.1778 - lr: 1.0000e-04\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 37s 887ms/step - loss: 1037.0833 - mae: 20.8924 - val_loss: 924.2814 - val_mae: 19.1803 - lr: 1.0000e-04\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 41s 972ms/step - loss: 1036.8243 - mae: 20.9132 - val_loss: 924.1006 - val_mae: 19.1856 - lr: 1.0000e-04\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 38s 896ms/step - loss: 1036.7991 - mae: 20.9092 - val_loss: 923.6942 - val_mae: 19.1450 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 36s 849ms/step - loss: 1036.7034 - mae: 20.9485 - val_loss: 923.3699 - val_mae: 19.1173 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 42s 1s/step - loss: 1036.3923 - mae: 20.9337 - val_loss: 923.4326 - val_mae: 19.1778 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 36s 859ms/step - loss: 1036.2631 - mae: 20.8511 - val_loss: 923.1224 - val_mae: 19.1556 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 35s 834ms/step - loss: 1036.2332 - mae: 20.8582 - val_loss: 922.9869 - val_mae: 19.1702 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 36s 848ms/step - loss: 1035.9164 - mae: 20.8927 - val_loss: 922.6883 - val_mae: 19.1518 - lr: 1.0000e-04\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 36s 850ms/step - loss: 1036.0496 - mae: 20.9503 - val_loss: 922.2915 - val_mae: 19.1088 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 35s 842ms/step - loss: 1035.4904 - mae: 20.8619 - val_loss: 922.0785 - val_mae: 19.1070 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 35s 826ms/step - loss: 1035.6737 - mae: 20.9041 - val_loss: 922.3812 - val_mae: 19.2061 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 35s 832ms/step - loss: 1035.3005 - mae: 20.9556 - val_loss: 921.6876 - val_mae: 19.1107 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 35s 834ms/step - loss: 1035.2003 - mae: 20.8082 - val_loss: 921.4909 - val_mae: 19.1139 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 42s 1s/step - loss: 1035.0446 - mae: 20.9024 - val_loss: 921.1869 - val_mae: 19.0879 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 34s 820ms/step - loss: 1034.8220 - mae: 20.8934 - val_loss: 920.9391 - val_mae: 19.0739 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 34s 810ms/step - loss: 1034.7250 - mae: 20.9095 - val_loss: 920.7980 - val_mae: 19.0907 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 34s 820ms/step - loss: 1034.4957 - mae: 20.8292 - val_loss: 920.6068 - val_mae: 19.0937 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 35s 831ms/step - loss: 1034.2563 - mae: 20.8490 - val_loss: 920.3169 - val_mae: 19.0692 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 35s 838ms/step - loss: 1034.1248 - mae: 20.8685 - val_loss: 920.2093 - val_mae: 19.0925 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 36s 847ms/step - loss: 1033.9318 - mae: 20.8106 - val_loss: 920.0727 - val_mae: 19.1068 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 34s 810ms/step - loss: 1033.7513 - mae: 20.8673 - val_loss: 919.7832 - val_mae: 19.0848 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 33s 789ms/step - loss: 1033.6760 - mae: 20.8644 - val_loss: 919.6158 - val_mae: 19.0923 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 32s 758ms/step - loss: 1033.4220 - mae: 20.8455 - val_loss: 919.5461 - val_mae: 19.1177 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 33s 774ms/step - loss: 1033.4216 - mae: 20.8475 - val_loss: 919.0123 - val_mae: 19.0321 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 32s 764ms/step - loss: 1033.0920 - mae: 20.8399 - val_loss: 919.2964 - val_mae: 19.1425 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 33s 779ms/step - loss: 1032.9618 - mae: 20.8548 - val_loss: 918.9111 - val_mae: 19.1059 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 32s 764ms/step - loss: 1032.8159 - mae: 20.8318 - val_loss: 918.5920 - val_mae: 19.0783 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 34s 814ms/step - loss: 1032.7126 - mae: 20.8591 - val_loss: 918.5704 - val_mae: 19.1125 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 33s 785ms/step - loss: 1032.5616 - mae: 20.8095 - val_loss: 918.2127 - val_mae: 19.0764 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 34s 796ms/step - loss: 1032.4594 - mae: 20.9047 - val_loss: 917.9061 - val_mae: 19.0493 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 33s 789ms/step - loss: 1032.1707 - mae: 20.7912 - val_loss: 917.6966 - val_mae: 19.0430 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 34s 816ms/step - loss: 1032.0779 - mae: 20.7978 - val_loss: 917.7465 - val_mae: 19.0958 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 33s 791ms/step - loss: 1031.9802 - mae: 20.8466 - val_loss: 917.3156 - val_mae: 19.0390 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 34s 802ms/step - loss: 1031.8218 - mae: 20.8616 - val_loss: 917.3879 - val_mae: 19.0964 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 33s 792ms/step - loss: 1031.6124 - mae: 20.7373 - val_loss: 916.9532 - val_mae: 19.0413 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 32s 768ms/step - loss: 1031.6404 - mae: 20.8977 - val_loss: 916.8455 - val_mae: 19.0587 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 33s 777ms/step - loss: 1031.3877 - mae: 20.8190 - val_loss: 916.5387 - val_mae: 19.0284 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 32s 767ms/step - loss: 1031.2286 - mae: 20.8522 - val_loss: 916.4212 - val_mae: 19.0436 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 34s 814ms/step - loss: 1030.9249 - mae: 20.7595 - val_loss: 916.1404 - val_mae: 19.0169 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 33s 792ms/step - loss: 1030.8405 - mae: 20.7839 - val_loss: 916.4607 - val_mae: 19.1198 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 33s 794ms/step - loss: 1031.0946 - mae: 20.8786 - val_loss: 916.3204 - val_mae: 19.1235 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 34s 808ms/step - loss: 1030.8855 - mae: 20.7724 - val_loss: 916.1021 - val_mae: 19.1163 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 34s 810ms/step - loss: 1030.2633 - mae: 20.8240 - val_loss: 915.5374 - val_mae: 19.0407 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 33s 798ms/step - loss: 1030.2202 - mae: 20.8160 - val_loss: 915.1754 - val_mae: 18.9896 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 33s 787ms/step - loss: 1030.0745 - mae: 20.7475 - val_loss: 915.2773 - val_mae: 19.0579 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 34s 807ms/step - loss: 1029.9327 - mae: 20.8379 - val_loss: 915.1588 - val_mae: 19.0667 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 33s 776ms/step - loss: 1030.0247 - mae: 20.7922 - val_loss: 915.0408 - val_mae: 19.0752 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 34s 811ms/step - loss: 1029.5034 - mae: 20.7917 - val_loss: 914.5635 - val_mae: 19.0094 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 34s 808ms/step - loss: 1029.4086 - mae: 20.7339 - val_loss: 914.3517 - val_mae: 18.9978 - lr: 1.0000e-04\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 35s 828ms/step - loss: 1029.5608 - mae: 20.8299 - val_loss: 914.2646 - val_mae: 19.0169 - lr: 1.0000e-04\n",
      "Epoch 94/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 34s 815ms/step - loss: 1029.2583 - mae: 20.7919 - val_loss: 914.0493 - val_mae: 19.0051 - lr: 1.0000e-04\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 34s 799ms/step - loss: 1029.0319 - mae: 20.7672 - val_loss: 913.8217 - val_mae: 18.9879 - lr: 1.0000e-04\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 35s 830ms/step - loss: 1028.8665 - mae: 20.7967 - val_loss: 913.7665 - val_mae: 19.0153 - lr: 1.0000e-04\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 35s 838ms/step - loss: 1029.1301 - mae: 20.8353 - val_loss: 913.3449 - val_mae: 18.9411 - lr: 1.0000e-04\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 34s 815ms/step - loss: 1028.5894 - mae: 20.7412 - val_loss: 913.3864 - val_mae: 19.0014 - lr: 1.0000e-04\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 34s 819ms/step - loss: 1028.6034 - mae: 20.8340 - val_loss: 913.4133 - val_mae: 19.0414 - lr: 1.0000e-04\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 34s 818ms/step - loss: 1028.2604 - mae: 20.7182 - val_loss: 913.0267 - val_mae: 18.9925 - lr: 1.0000e-04\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 35s 833ms/step - loss: 1028.1750 - mae: 20.7784 - val_loss: 912.8292 - val_mae: 18.9828 - lr: 1.0000e-04\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 34s 818ms/step - loss: 1028.0192 - mae: 20.7132 - val_loss: 912.5765 - val_mae: 18.9570 - lr: 1.0000e-04\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 35s 841ms/step - loss: 1028.0048 - mae: 20.7903 - val_loss: 912.4410 - val_mae: 18.9637 - lr: 1.0000e-04\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 34s 799ms/step - loss: 1027.8174 - mae: 20.7821 - val_loss: 912.5602 - val_mae: 19.0277 - lr: 1.0000e-04\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 34s 814ms/step - loss: 1027.8052 - mae: 20.8156 - val_loss: 912.3924 - val_mae: 19.0231 - lr: 1.0000e-04\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 34s 814ms/step - loss: 1027.7220 - mae: 20.6682 - val_loss: 912.3759 - val_mae: 19.0484 - lr: 1.0000e-04\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 34s 817ms/step - loss: 1027.3506 - mae: 20.7954 - val_loss: 911.9054 - val_mae: 18.9843 - lr: 1.0000e-04\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 36s 857ms/step - loss: 1027.0338 - mae: 20.7635 - val_loss: 911.9749 - val_mae: 19.0299 - lr: 1.0000e-04\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 39s 920ms/step - loss: 1027.0236 - mae: 20.7714 - val_loss: 911.6523 - val_mae: 18.9950 - lr: 1.0000e-04\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 36s 856ms/step - loss: 1026.7769 - mae: 20.7922 - val_loss: 911.5091 - val_mae: 18.9978 - lr: 1.0000e-04\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 35s 847ms/step - loss: 1026.8369 - mae: 20.7621 - val_loss: 911.4008 - val_mae: 19.0061 - lr: 1.0000e-04\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 35s 834ms/step - loss: 1026.5328 - mae: 20.7112 - val_loss: 910.8983 - val_mae: 18.9161 - lr: 1.0000e-04\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 35s 836ms/step - loss: 1026.3551 - mae: 20.7258 - val_loss: 911.1155 - val_mae: 19.0081 - lr: 1.0000e-04\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 35s 832ms/step - loss: 1026.3344 - mae: 20.7078 - val_loss: 910.9608 - val_mae: 19.0070 - lr: 1.0000e-04\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 35s 828ms/step - loss: 1026.0902 - mae: 20.7335 - val_loss: 910.8204 - val_mae: 19.0081 - lr: 1.0000e-04\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 34s 803ms/step - loss: 1026.2380 - mae: 20.8308 - val_loss: 910.6870 - val_mae: 19.0103 - lr: 1.0000e-04\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 36s 853ms/step - loss: 1026.1060 - mae: 20.6418 - val_loss: 910.2115 - val_mae: 18.9375 - lr: 1.0000e-04\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 35s 827ms/step - loss: 1025.8329 - mae: 20.7534 - val_loss: 910.2025 - val_mae: 18.9706 - lr: 1.0000e-04\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 35s 841ms/step - loss: 1025.8428 - mae: 20.8004 - val_loss: 910.0767 - val_mae: 18.9743 - lr: 1.0000e-04\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 35s 843ms/step - loss: 1025.6227 - mae: 20.6401 - val_loss: 909.9637 - val_mae: 18.9814 - lr: 1.0000e-04\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 34s 810ms/step - loss: 1025.6895 - mae: 20.8649 - val_loss: 909.9987 - val_mae: 19.0148 - lr: 1.0000e-04\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 34s 817ms/step - loss: 1025.0625 - mae: 20.6640 - val_loss: 909.2190 - val_mae: 18.8420 - lr: 1.0000e-04\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 41s 993ms/step - loss: 1025.0154 - mae: 20.6819 - val_loss: 909.6848 - val_mae: 19.0088 - lr: 1.0000e-04\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 34s 812ms/step - loss: 1024.8445 - mae: 20.7727 - val_loss: 909.1684 - val_mae: 18.9299 - lr: 1.0000e-04\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 35s 839ms/step - loss: 1025.2865 - mae: 20.7307 - val_loss: 909.3226 - val_mae: 18.9946 - lr: 1.0000e-04\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 35s 844ms/step - loss: 1024.7607 - mae: 20.6309 - val_loss: 908.6547 - val_mae: 18.8566 - lr: 1.0000e-04\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 34s 804ms/step - loss: 1024.8081 - mae: 20.7725 - val_loss: 908.6869 - val_mae: 18.9141 - lr: 1.0000e-04\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 34s 821ms/step - loss: 1024.3344 - mae: 20.6719 - val_loss: 908.7006 - val_mae: 18.9520 - lr: 1.0000e-04\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 33s 796ms/step - loss: 1024.2886 - mae: 20.7315 - val_loss: 908.4417 - val_mae: 18.9243 - lr: 1.0000e-04\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 34s 801ms/step - loss: 1024.1790 - mae: 20.6920 - val_loss: 908.5394 - val_mae: 18.9754 - lr: 1.0000e-04\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 35s 827ms/step - loss: 1024.0116 - mae: 20.7125 - val_loss: 908.1633 - val_mae: 18.9239 - lr: 1.0000e-04\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 36s 854ms/step - loss: 1023.8333 - mae: 20.6525 - val_loss: 908.3958 - val_mae: 18.9992 - lr: 1.0000e-04\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 32s 770ms/step - loss: 1023.6799 - mae: 20.7930 - val_loss: 908.0554 - val_mae: 18.9599 - lr: 1.0000e-04\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 33s 795ms/step - loss: 1023.8040 - mae: 20.7130 - val_loss: 908.0913 - val_mae: 18.9914 - lr: 1.0000e-04\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 35s 841ms/step - loss: 1023.5361 - mae: 20.6480 - val_loss: 907.5728 - val_mae: 18.9126 - lr: 1.0000e-04\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 34s 807ms/step - loss: 1023.3262 - mae: 20.7364 - val_loss: 908.0275 - val_mae: 19.0238 - lr: 1.0000e-04\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 34s 812ms/step - loss: 1023.2765 - mae: 20.7006 - val_loss: 907.5524 - val_mae: 18.9661 - lr: 1.0000e-04\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 35s 837ms/step - loss: 1022.9866 - mae: 20.6980 - val_loss: 906.9738 - val_mae: 18.8583 - lr: 1.0000e-04\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 33s 799ms/step - loss: 1022.9763 - mae: 20.6959 - val_loss: 906.9831 - val_mae: 18.9007 - lr: 1.0000e-04\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 35s 835ms/step - loss: 1022.8516 - mae: 20.7042 - val_loss: 906.7660 - val_mae: 18.8776 - lr: 1.0000e-04\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 34s 808ms/step - loss: 1022.6060 - mae: 20.6613 - val_loss: 906.6919 - val_mae: 18.8944 - lr: 1.0000e-04\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 33s 797ms/step - loss: 1022.4830 - mae: 20.6640 - val_loss: 906.8467 - val_mae: 18.9577 - lr: 1.0000e-04\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 34s 811ms/step - loss: 1022.4572 - mae: 20.6686 - val_loss: 906.7215 - val_mae: 18.9593 - lr: 1.0000e-04\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 33s 794ms/step - loss: 1022.4498 - mae: 20.7374 - val_loss: 906.6452 - val_mae: 18.9687 - lr: 1.0000e-04\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 34s 803ms/step - loss: 1022.0961 - mae: 20.7024 - val_loss: 906.0086 - val_mae: 18.8527 - lr: 1.0000e-04\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 35s 827ms/step - loss: 1022.5036 - mae: 20.6563 - val_loss: 906.6929 - val_mae: 19.0177 - lr: 1.0000e-04\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 35s 824ms/step - loss: 1021.9520 - mae: 20.7392 - val_loss: 905.9300 - val_mae: 18.9028 - lr: 1.0000e-04\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 35s 824ms/step - loss: 1021.8475 - mae: 20.6003 - val_loss: 905.5890 - val_mae: 18.8464 - lr: 1.0000e-04\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 34s 811ms/step - loss: 1021.7117 - mae: 20.7502 - val_loss: 905.8625 - val_mae: 18.9427 - lr: 1.0000e-04\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 34s 807ms/step - loss: 1021.9720 - mae: 20.5523 - val_loss: 905.6797 - val_mae: 18.9326 - lr: 1.0000e-04\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 34s 799ms/step - loss: 1021.4662 - mae: 20.7677 - val_loss: 905.8647 - val_mae: 18.9864 - lr: 1.0000e-04\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 33s 798ms/step - loss: 1021.4216 - mae: 20.6948 - val_loss: 905.0594 - val_mae: 18.8439 - lr: 1.0000e-04\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 34s 801ms/step - loss: 1021.1296 - mae: 20.6317 - val_loss: 905.2393 - val_mae: 18.9198 - lr: 1.0000e-04\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 34s 809ms/step - loss: 1021.1964 - mae: 20.7413 - val_loss: 905.0795 - val_mae: 18.9115 - lr: 1.0000e-04\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 34s 799ms/step - loss: 1021.1094 - mae: 20.6662 - val_loss: 905.0140 - val_mae: 18.9236 - lr: 1.0000e-04\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 33s 793ms/step - loss: 1020.8453 - mae: 20.6523 - val_loss: 904.7633 - val_mae: 18.8979 - lr: 1.0000e-04\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 34s 808ms/step - loss: 1020.6403 - mae: 20.6651 - val_loss: 904.6179 - val_mae: 18.8933 - lr: 1.0000e-04\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 35s 829ms/step - loss: 1020.5723 - mae: 20.6418 - val_loss: 904.5470 - val_mae: 18.9038 - lr: 1.0000e-04\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 34s 810ms/step - loss: 1020.5284 - mae: 20.6802 - val_loss: 904.4113 - val_mae: 18.9009 - lr: 1.0000e-04\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 33s 796ms/step - loss: 1020.3912 - mae: 20.6731 - val_loss: 904.5398 - val_mae: 18.9478 - lr: 1.0000e-04\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 33s 789ms/step - loss: 1020.3559 - mae: 20.6492 - val_loss: 903.7429 - val_mae: 18.7753 - lr: 1.0000e-04\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 33s 795ms/step - loss: 1020.3584 - mae: 20.7194 - val_loss: 903.8201 - val_mae: 18.8475 - lr: 1.0000e-04\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 33s 791ms/step - loss: 1020.1641 - mae: 20.5099 - val_loss: 903.6724 - val_mae: 18.8402 - lr: 1.0000e-04\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 34s 802ms/step - loss: 1019.8002 - mae: 20.7145 - val_loss: 904.4177 - val_mae: 19.0011 - lr: 1.0000e-04\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 33s 791ms/step - loss: 1019.8109 - mae: 20.6886 - val_loss: 903.5140 - val_mae: 18.8609 - lr: 1.0000e-04\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 34s 818ms/step - loss: 1019.8165 - mae: 20.6306 - val_loss: 903.4290 - val_mae: 18.8696 - lr: 1.0000e-04\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 34s 822ms/step - loss: 1019.6559 - mae: 20.6480 - val_loss: 903.3586 - val_mae: 18.8794 - lr: 1.0000e-04\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 33s 789ms/step - loss: 1019.4744 - mae: 20.6434 - val_loss: 903.4879 - val_mae: 18.9271 - lr: 1.0000e-04\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 34s 809ms/step - loss: 1019.2610 - mae: 20.6368 - val_loss: 903.1789 - val_mae: 18.8895 - lr: 1.0000e-04\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 33s 792ms/step - loss: 1019.1676 - mae: 20.6488 - val_loss: 903.2382 - val_mae: 18.9218 - lr: 1.0000e-04\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 33s 790ms/step - loss: 1019.3748 - mae: 20.5783 - val_loss: 903.2741 - val_mae: 18.9475 - lr: 1.0000e-04\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 34s 802ms/step - loss: 1019.4344 - mae: 20.8034 - val_loss: 902.5461 - val_mae: 18.8195 - lr: 1.0000e-04\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 34s 814ms/step - loss: 1018.9088 - mae: 20.5779 - val_loss: 902.5156 - val_mae: 18.8416 - lr: 1.0000e-04\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 36s 859ms/step - loss: 1018.6776 - mae: 20.6115 - val_loss: 902.5334 - val_mae: 18.8710 - lr: 1.0000e-04\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 39s 923ms/step - loss: 1018.6054 - mae: 20.7138 - val_loss: 902.4670 - val_mae: 18.8808 - lr: 1.0000e-04\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 38s 914ms/step - loss: 1018.5989 - mae: 20.5680 - val_loss: 902.2905 - val_mae: 18.8667 - lr: 1.0000e-04\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 37s 877ms/step - loss: 1018.5452 - mae: 20.6751 - val_loss: 902.3018 - val_mae: 18.8912 - lr: 1.0000e-04\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 37s 879ms/step - loss: 1018.3663 - mae: 20.6185 - val_loss: 901.6599 - val_mae: 18.7383 - lr: 1.0000e-04\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 37s 879ms/step - loss: 1018.3028 - mae: 20.6096 - val_loss: 901.8438 - val_mae: 18.8394 - lr: 1.0000e-04\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 34s 821ms/step - loss: 1018.0911 - mae: 20.6503 - val_loss: 901.8279 - val_mae: 18.8606 - lr: 1.0000e-04\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 37s 880ms/step - loss: 1018.0391 - mae: 20.5710 - val_loss: 901.7781 - val_mae: 18.8728 - lr: 1.0000e-04\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 34s 811ms/step - loss: 1017.9757 - mae: 20.7023 - val_loss: 901.3280 - val_mae: 18.7870 - lr: 1.0000e-04\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 33s 795ms/step - loss: 1017.8427 - mae: 20.5666 - val_loss: 901.2302 - val_mae: 18.7900 - lr: 1.0000e-04\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 33s 792ms/step - loss: 1017.6971 - mae: 20.6841 - val_loss: 901.4100 - val_mae: 18.8628 - lr: 1.0000e-04\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 34s 813ms/step - loss: 1017.9306 - mae: 20.6215 - val_loss: 901.0776 - val_mae: 18.8091 - lr: 1.0000e-04\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 33s 798ms/step - loss: 1017.4084 - mae: 20.5556 - val_loss: 901.0068 - val_mae: 18.8186 - lr: 1.0000e-04\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 34s 803ms/step - loss: 1017.4225 - mae: 20.7063 - val_loss: 901.1012 - val_mae: 18.8632 - lr: 1.0000e-04\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 34s 806ms/step - loss: 1017.6182 - mae: 20.5434 - val_loss: 900.9388 - val_mae: 18.8501 - lr: 1.0000e-04\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 35s 838ms/step - loss: 1017.3821 - mae: 20.6980 - val_loss: 900.4720 - val_mae: 18.7482 - lr: 1.0000e-04\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 36s 865ms/step - loss: 1017.0602 - mae: 20.5317 - val_loss: 900.5251 - val_mae: 18.8014 - lr: 1.0000e-04\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 36s 867ms/step - loss: 1017.0773 - mae: 20.6912 - val_loss: 900.6208 - val_mae: 18.8493 - lr: 1.0000e-04\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 36s 856ms/step - loss: 1016.9906 - mae: 20.5973 - val_loss: 900.3231 - val_mae: 18.8015 - lr: 1.0000e-04\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 35s 842ms/step - loss: 1016.8438 - mae: 20.6166 - val_loss: 900.1905 - val_mae: 18.7924 - lr: 1.0000e-04\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 35s 833ms/step - loss: 1016.5778 - mae: 20.5969 - val_loss: 900.2259 - val_mae: 18.8269 - lr: 1.0000e-04\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 36s 869ms/step - loss: 1016.5709 - mae: 20.5881 - val_loss: 900.1581 - val_mae: 18.8345 - lr: 1.0000e-04\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 37s 886ms/step - loss: 1016.7417 - mae: 20.6886 - val_loss: 899.8438 - val_mae: 18.7805 - lr: 1.0000e-04\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 36s 854ms/step - loss: 1016.5071 - mae: 20.5329 - val_loss: 899.9520 - val_mae: 18.8334 - lr: 1.0000e-04\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 37s 879ms/step - loss: 1016.2105 - mae: 20.6112 - val_loss: 899.9920 - val_mae: 18.8608 - lr: 1.0000e-04\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 37s 886ms/step - loss: 1016.2108 - mae: 20.6879 - val_loss: 899.5267 - val_mae: 18.7737 - lr: 1.0000e-04\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 34s 821ms/step - loss: 1016.2034 - mae: 20.5704 - val_loss: 899.8089 - val_mae: 18.8617 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# from sst and rainfall ---> rainfall\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Conv2D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import Attention\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Lambda\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2DTranspose, TimeDistributed\n",
    "from keras import callbacks\n",
    "import xarray as xr\n",
    "\n",
    "# Define global variables\n",
    "lead_time = 2\n",
    "save_path = \"/home/cccr/roxy/matin/MTech_project/model/Conv-LSTM/7in3out/\"\n",
    "model_path = save_path + \"ConvLstm_sst0rf_rf_2bob.h5\"\n",
    "add_data = \"/home/cccr/roxy/matin/MTech_project/data/\"\n",
    "\n",
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"1998-01-01\",\"2016-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "channels = [\"FilteredrfBOB_0lag.nc\",\"FilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"FilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n",
    "\n",
    "# Perform additional processing or modeling steps as needed\n",
    "seq = tf.keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "seq.add(ConvLSTM2D(filters=4, kernel_size=(3,3), padding='same', input_shape=(7,25,39,2), return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=8, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=True, data_format='channels_last'))\n",
    "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3), padding='same', return_sequences=False, data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=15, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "seq.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', padding='same', data_format='channels_last'))\n",
    "# Create the query, value, and key inputs\n",
    "query_input = Input(shape=(25,39,2), name='query_input')\n",
    "value_input = Input(shape=(25,39,2), name='value_input')\n",
    "key_input = Input(shape=(25,39,2), name='key_input')\n",
    "\n",
    "# Pass the inputs to the Attention layer\n",
    "attention_output = Attention(name='attention')([query_input, value_input, key_input])\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=10**-4)\n",
    "seq.compile(loss='mean_squared_error', optimizer=Adam, metrics=['mae'])\n",
    "\n",
    "print(seq.summary())\n",
    "\n",
    "# Define the number of training epochs\n",
    "n_epochs = 200\n",
    "logdir = save_path\n",
    "# Define the TensorBoard callback for monitoring training progress\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=10, batch_size=10, write_graph=True)\n",
    "\n",
    "# Define the ReduceLROnPlateau callback for reducing the learning rate when the model plateaus\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=20, min_lr=10**-20)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation mean absolute error\n",
    "checkpoint_callback_mae = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_mae',\n",
    "                                                             mode='min', save_best_only=True)\n",
    "\n",
    "# Define the ModelCheckpoint callback for saving the best model based on validation loss\n",
    "checkpoint_callback_loss = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss',\n",
    "                                                              mode='min', save_best_only=True)\n",
    "\n",
    "# Define the EarlyStopping callback for stopping training when the model stops improving\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "# Define the TerminateOnNaN callback for terminating the training process if there is NaN value in any of the output\n",
    "terminate_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "# Train the model using the defined callbacks\n",
    "history = seq.fit(x, y, epochs=n_epochs, validation_split=0.1,\n",
    "                  batch_size=50, callbacks=[early_stop_callback,\n",
    "                  checkpoint_callback_mae, checkpoint_callback_loss,\n",
    "                  tb_callback, lr_callback, terminate_callback])\n",
    "#Save the trained model\n",
    "seq.save(model_path)\n",
    "\n",
    "#Save the training history\n",
    "\n",
    "np.save(f'{save_path}ConvLstm_sst0rf_rf_2bob.npy', history.history)               \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3753d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding channel 0 with shape: (235, 7, 1, 25, 39)\n",
      "Adding channel 1 with shape: (235, 7, 1, 25, 39)\n",
      "INPUT SHAPE -->  (235, 7, 2, 25, 39)\n",
      "TARGET SHAPE -->  (235, 1, 25, 39)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(sequence, n_steps, channels):\n",
    "    \"\"\"\n",
    "    Function to preprocess the data and prepare it for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        sequence (xarray.Dataset): Data sequence to be preprocessed.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        channels (int): Number of channels in the input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the preprocessed input sequence (X) and output sequence (y).\n",
    "    \"\"\"\n",
    "    # Select data for Jun, Jul, Aug, Sept\n",
    "    sequence = sequence.sel(time=slice(\"2017-01-01\",\"2018-12-31\"))\n",
    "    sequence = sequence.where(sequence.time.dt.month.isin([6,7,8,9]), drop=True)\n",
    "    max = sequence.max()\n",
    "    min = sequence.min()\n",
    "    \n",
    "    # Max-min normalization\n",
    "    sequence = (sequence-min)/(max - min)\n",
    "    sequence = np.array(sequence)\n",
    "    \n",
    "    # Exponential Space Transform\n",
    "    valid_pts = np.where(sequence==sequence)\n",
    "    sequence[valid_pts] = np.exp(sequence[valid_pts])\n",
    "    sequence[valid_pts] = np.power(sequence[valid_pts],7)\n",
    "    invalid_pts = np.where(sequence != sequence)\n",
    "    sequence[invalid_pts] = 0\n",
    "    \n",
    "    # Prepare input and output sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix + lead_time > len(sequence)-1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + lead_time]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_input(channels, n_steps):\n",
    "    \"\"\"\n",
    "    Function to create the input for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        channels (list): List of channels to be used in the input.\n",
    "        n_steps (int): Number of time steps in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The input for the model.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for i in range(len(channels)):\n",
    "        channel_data = xr.open_dataarray(f\"/home/cccr/roxy/matin/MTech_project/data/{channels[i]}\")\n",
    "        input, _ = preprocess_data(channel_data, n_steps=n_steps, channels=channels[i])\n",
    "        input = np.expand_dims(input, axis=2)\n",
    "        print(f\"Adding channel {i} with shape: {input.shape}\")\n",
    "        stack.append(input)\n",
    "    out = np.dstack(stack)\n",
    "    return out\n",
    "\n",
    "\n",
    "channels = [\"FilteredrfBOB_0lag.nc\",\"FilteredSSTBOB_0.nc\"]\n",
    "steps = 7\n",
    "\n",
    "# Create input for the model\n",
    "input = create_input(channels, steps)\n",
    "\n",
    "# Preprocess target data\n",
    "channel_data = xr.open_dataarray(add_data + \"FilteredrfBOB_0lag.nc\")\n",
    "_, target = preprocess_data(channel_data, n_steps=steps, channels=channels[-1])\n",
    "target = np.expand_dims(target, axis=1)\n",
    "\n",
    "# Roll axes for input and target\n",
    "x = np.rollaxis(input, 4, 2)\n",
    "x = np.rollaxis(x, 4, 2)\n",
    "\n",
    "y = np.rollaxis(target, 3, 1)\n",
    "y = np.rollaxis(y, 3, 1)\n",
    "\n",
    "# Assert that input and target shapes are consistent\n",
    "assert input.shape[0] == target.shape[0]\n",
    "assert input.shape[-1] == target.shape[-1]\n",
    "assert input.shape[-2] == target.shape[-2]\n",
    "\n",
    "\n",
    "# Print shape of input and target\n",
    "print(\"INPUT SHAPE --> \", input.shape)\n",
    "print(\"TARGET SHAPE --> \", target.shape)\n",
    "\n",
    "# Delete original input and target variables to free up memory\n",
    "del input\n",
    "del target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7bb9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 92ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.05160796],\n",
       "       [-0.05160796,  1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(model_path)\n",
    "pred = model.predict(x)\n",
    "\n",
    "actual = y.flatten()\n",
    "prediction = pred.flatten()\n",
    "corr = np.corrcoef(actual,prediction)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1474e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
